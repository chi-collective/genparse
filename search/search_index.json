{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#genparse","title":"GenParse","text":"<p>GenParse is a Python library for constrained generation with language models, specialized for tasks like semantic parsing and code generation. It uses sequential Monte Carlo (SMC) inference to ensure that language model generations comply with user-defined syntactic and semantic constraints. The library is equipped with proposal distributions that efficiently enforce syntactic constraints, supports constraints beyond grammaticality through arbitrary scoring (potential) functions, and is integrated with vLLM for fast language model inference.</p> <p>\u26a0\ufe0f Warning: This library is currently in active development. We recommend frequently pulling the latest version to stay updated with improvements and bug fixes. Please report any bugs in the issue tracker.</p> <p>First time here? Go to our Full Documentation. </p>"},{"location":"#installation","title":"Installation","text":"<p>This library supports an automated build using GNU Make.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 - 3.12</li> <li>pip (Python package installer)</li> <li>make</li> <li>git</li> <li>A GPU with compute capability 7.0 or higher (GPUs are not required, but strongly recommended)</li> </ul>"},{"location":"#steps","title":"Steps","text":""},{"location":"#1-clone-this-repository","title":"1. Clone this repository:","text":"<pre><code>git clone git@github.com:probcomp/genparse.git\ncd genparse\n</code></pre>"},{"location":"#2-create-and-activate-a-virtual-environment-using-conda-recommended","title":"2. Create and activate a virtual environment. Using Conda (recommended):","text":"<pre><code>conda create -n genparse python=3.10\nconda activate genparse\n</code></pre> <p>Using Python's <code>venv</code> module:</p> <pre><code>python -m venv genparse\nsource genparse/bin/activate  \n</code></pre> <p>\ud83d\udca1Tip: On Windows, use <code>genparse\\Scripts\\activate</code></p>"},{"location":"#3-install-package-in-editable-mode-with-pre-commit-hooks","title":"3. Install package in editable mode with pre-commit hooks","text":"<pre><code>make env \n</code></pre> <p>GenParse optionally depends on Rust for faster parsing. If you do not have Rust installed, you will prompted to do so. However, if you do not want to install Rust, you can also install the library without the Rust dependency via:</p> <pre><code>make env-no-rust\n</code></pre>"},{"location":"#4-you-can-test-your-installation-by-running-the-following-example","title":"4. You can test your installation by running the following example:","text":"<pre><code>&gt;&gt;&gt; from genparse import InferenceSetup\n&gt;&gt;&gt; grammar = 'start: \"Sequential Monte Carlo is \" ( \"good\" | \"bad\" )'\n&gt;&gt;&gt; m = InferenceSetup('gpt2', grammar, proposal_name='character')\n&gt;&gt;&gt; m(' ', n_particles=15)\n{\n   'Sequential Monte Carlo is good\u25aa': 0.7770842914205952,\n   'Sequential Monte Carlo is bad\u25aa': 0.22291570857940482,\n}\n</code></pre> <p>Or by running:</p> <pre><code>python examples/genparse_tiny_example.py\n</code></pre>"},{"location":"#supported-language-models","title":"Supported language models","text":"<p>Genparse currently supports the following HuggingFace language models. If you would like support for an additional model, please create an issue. </p> Name HuggingFace Identifier llama3 meta-llama/Meta-Llama-3-8B llama3.1 meta-llama/Meta-Llama-3.1-8B llama3-instruct meta-llama/Meta-Llama-3-8B-Instruct llama3.1-instruct meta-llama/Meta-Llama-3.1-8B-Instruct codellama codellama/CodeLlama-7b-Instruct-hf gpt2 gpt2 gpt2-medium gpt2-medium gpt2-large gpt2-large llama3.2-1B meta-llama/Meta-Llama-3.2-1B <p>Adding a <code>mock-</code> prefix to a language model name will create an imitation language model over the same vocabulary that can be used for testing (e.g., <code>mock-gpt2</code>). In practice, these models can be useful for rapid prototyping with minimal hardware.</p>"},{"location":"#development","title":"Development","text":"<p>After installation, you can use the following commands for development:</p> <ul> <li><code>make help</code>: Print available commands</li> <li><code>make update</code>: Update the repository from GitHub</li> <li><code>make format</code>: Format code style using ruff</li> <li><code>make docs</code>: Build documentation using pdoc</li> <li><code>make mkdocs</code>: Build full documentation using mkdocs</li> <li><code>make test</code>: Run linting (ruff) and tests (pytest with coverage)</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>Thanks to VLLM, Lark, Hugging Face and all of the teams we have dependencies on.</p>"},{"location":"#licensing","title":"Licensing","text":"<p>This project makes use of several open-source dependencies. We have done our best to represent this correctly, but please do your own due diligence:</p> Library License Arsenal GNU General Public License v3.0 FrozenDict MIT License Graphviz MIT License Interegular MIT License IPython BSD 3-Clause \"New\" or \"Revised\" License Jsons MIT License Lark MIT License NLTK Apache 2.0 License NumPy BSD License Pandas BSD 3-Clause \"New\" or \"Revised\" License Path MIT License Rich MIT License NumBa BSD 2-Clause \"Simplified\" License Torch (PyTorch) BSD License Transformers Apache 2.0 License Plotly MIT License Maturin Apache 2.0 License OR MIT License Psutil BSD 3-Clause \"New\" or \"Revised\" License MkDocs BSD 2-Clause \"Simplified\" License MkDocs Material MIT License <p>Each dependency retains its own license, which can be found in the respective repository or package distribution.</p>"},{"location":"#contact","title":"Contact","text":"<p>For questions and support, please open an issue on the GitHub repository.</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#development","title":"Development","text":"<p>After installation, you can use the following commands for development:</p> <ul> <li><code>make help</code>: Print available commands</li> <li><code>make update</code>: Update the repository from GitHub</li> <li><code>make format</code>: Format code style using ruff</li> <li><code>make docs</code>: Builds auto-generated API documentation using pdoc</li> <li><code>make mkdocs</code>: Build full documentation using mkdocs</li> <li><code>make test</code>: Run linting (ruff) and tests (pytest with coverage)</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Before pushing a new commit, always run:</p> <pre><code>make test\n</code></pre> <p>This will run all tests and ensure code quality.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>To build the auto-generated API documentation, run:</p> <pre><code>make docs\n</code></pre> <p>To build the mkdocs documentation, run:</p> <pre><code>make mkdocs\n</code></pre> <p>\ud83d\udca1Tip: Note that the documentation index is symlinked from the README.md file. If you are on a Windows machine you will need to manually symlink the README.md file to docs/index.md before building the docs.</p> <p>mkdocs takes documentation in the /docs directory and builds a static html version of it, which it puts into /site. When PRs are approved and merged the docs are rebuilt by a github action and deployed to the genparse.gen.dev domain. </p> <p>If you want to test the docs on your own branch, run:</p> <pre><code>serve mkdocs\n</code></pre> <p>The results will be served at localhost:8000.</p> <p>You can test a deployed version of the docs by pushing to a branch called mkdocs-branch. The github action will automatically deploy the branch to the genparse.dev domain. You can view the results of the action on github and also rerun the action there. </p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>GenParse currently provides a high-level interface for constrained generation via the <code>InferenceSetup</code> class. We recommend using this class as its internals may be deprecated without prior warning. </p> <pre><code>from genparse import InferenceSetup\n</code></pre>"},{"location":"getting_started/#1-define-your-grammar","title":"1. Define your grammar","text":"<p>GenParse uses Lark syntax for grammar specification. For example:</p> <pre><code>grammar = \"\"\"\nstart: WS? \"SELECT\" WS column WS from_clause (WS group_clause)?\nfrom_clause: \"FROM\" WS table\ngroup_clause: \"GROUP BY\" WS column\ncolumn: \"age\" | \"name\"\ntable: \"employees\"\nWS: \" \"\n\"\"\"\n</code></pre> <p>For a comprehensive guide on how to write grammars using Lark syntax, please refer to the official Lark documentation.</p> <p>\ud83d\udca1Tip: GenParse supports grammars with arbitrary regular expressions. In practice, we recommend avoiding extremely permisive regular expressions (e.g., <code>/.+/</code>) since these will lead to significantly slower inference. See issue #62.</p> <p>\ud83d\udca1Tip: If you don't allow your grammar to generate tokens that begin with a space, generation performance gets much worse. GenParse grammar requires adding a \" \" terminal to the top-level production rule.</p>"},{"location":"getting_started/#2-create-an-inferencesetup-object","title":"2. Create an <code>InferenceSetup</code> object","text":"<p>Create an <code>InferenceSetup</code> object with your chosen language model and grammar:</p> <pre><code>setup = InferenceSetup('gpt2', grammar)\n</code></pre> <p>Find the list of parameters for <code>InferenceSetup</code> in the pdocs API documentation here.</p> <p>\ud83d\udca1Tip: To try different grammars without having to instantiate new <code>InferenceSetup</code> objects each time, use the <code>update_grammar</code> method; <code>setup.update_grammar(new_grammar)</code> will replace the existing grammar in <code>setup</code> with <code>new_grammar</code>.</p> <p>\ud83d\udca1Tip: If you choose to use a Llama model you will need to authenticate with huggingface. You can do this by running <code>huggingface-cli login</code> and entering your credentials (a token), which you can get from here. You will also need to sign a waiver to use the Llama models, which you can do here. You may need to wait for approval before you can use the Llama models.</p>"},{"location":"getting_started/#3-run-inference","title":"3. Run inference","text":"<p>Use the setup object to run SMC inference:</p> <pre><code># The result is a ParticleApproximation object\nresult = setup('Write an SQL query:', n_particles=10, verbosity=1, max_tokens=25)\n</code></pre> <p>Find the list of parameters for the call method in the pdocs API documentation here.</p> <p>The result from <code>InferenceSetup</code> is a <code>ParticleApproximation</code> object. This object contains a collection of particles, each representing a generated text sequence. Each particle has two main attributes:</p> <ul> <li><code>context</code>: The generated text sequence.</li> <li><code>weight</code>: A numerical value representing the particle's importance weight. The weights are not normalized probabilities. GenParse provides post-processing to convert these weights into meaningful probabilities, which can be accessed via the <code>.posterior</code> property:</li> </ul> <pre><code>&gt;&gt;&gt; result.posterior\n{\"SELECT name FROM employees GROUP BY name\u25aa\" : 1}\n</code></pre>"},{"location":"getting_started/#4-potential-functions","title":"4. Potential functions","text":"<p>\ud83d\udca1 Tip: Incorporate constraints directly into the grammar when possible, as this will generally improve the quality of inference.</p> <p>Potential functions can be used to guide generation using additional constraints. A potential function maps (partial) generations to positive real numbers, with higher values indicating a stronger preference for those generations. Intuitively, when applied in SMC, potential functions offer richer signals for resampling steps, allowing computation to be redirected toward more promising particles during the course of generation.</p> <p>Potentials are provided as input to an <code>InferenceSetup</code> call via the <code>potential</code> argument and must be defined at the particle beam level. That is, <code>InferenceSetup</code> expects potentials to be callables which are provided a list of particles as input and return a list of log potential values, one for each particle. </p> <p>There is an example of a potential function in genparse_sql_example.py.</p>"},{"location":"getting_started/#5-visualizing-inference","title":"5. Visualizing inference","text":"<p>See the visualizing inference page for more details.</p>"},{"location":"jupyter_examples/","title":"Jupyter Examples","text":"<p>We are providing a Jupyter notebook that demonstrates a simple use case of the GenParse library for constrained text generation. This is a great way to walk through the code step-by-step and get a sense of how the library works.</p> <ul> <li>Genparse_tiny_example uses a basic grammar to generate completions for the phrase \"Sequential Monte Carlo is\", constraining the output to either \"good\" or \"bad\".</li> <li>Genparse_sql_example uses a SQL grammar to generate SQL queries.</li> </ul> <p>The notebooks showcase how to set up inference, run it, and process the results to obtain probabilities for each generated text.</p>"},{"location":"jupyter_examples/#installation","title":"Installation","text":"<p>Start by configuring a Conda environment within which we will install the dependencies for GenParse.</p> <pre><code>conda create -n genparse python=3.12\nconda activate genparse\n</code></pre> <p>In order to ensure your python installation shows up as a kernel in Jupyter Notebook you need to install a python kernel in this conda environment, and then name it so you can select it later in Jupyter Notebook.</p> <pre><code>conda install ipykernel # install a python kernel in this conda env\npython -m ipykernel install --user --name genparse --display-name \"Genparse Python Kernel\"\n</code></pre> <p>Set up the environment inside of Conda:</p> <pre><code>make env\n</code></pre> <p>Or, if you don't want to build the Rust parser, you can use the pre-built one:</p> <pre><code>make env_no_rust\n</code></pre> <p>\ud83d\udca1 Tip: If you run into problems with make env thinking it doesn't need to run, you can force it to install using:</p> <pre><code>make -B env-no-rust\n</code></pre> <p>If you haven't already installed Jupyter Notebook, you can do so using pip: </p> <pre><code>conda install -c conda-forge notebook ipywidgets jupyter_contrib_nbextensions\njupyter nbextension enable --py widgetsnbextension --sys-prefix # Ensure that ipywidgets is enabled for Jupyter Notebook\n</code></pre> <p>Jupyter defaults files to untrusted. You can trust a notebook by running:</p> <pre><code>jupyter trust genparse_tiny_example.ipynb\njupyter trust genparse_sql_example.ipynb\n</code></pre> <p>Start the Jupyter Notebook server by running:</p> <pre><code>jupyter notebook\n</code></pre> <p>This command will open a new tab in your default web browser with the Jupyter Notebook interface.</p> <p>\ud83d\udca1 Tip: In the web browser there will be a pop-up asking you to select your kernel. The default is \"Python 3 (ipykernel): and the other option is \"Genparse Python Kernel\". Select \"Genparse Python Kernel\" and set it to the default. If you enter and don't see a pop-up, check the upper right hand corner and ensure \"Genparse Python Kernel\" is selected.</p>"},{"location":"jupyter_examples/#run-the-notebook","title":"Run the Notebook","text":"<p>Once the notebook is open, you can run the cells sequentially by clicking on them and pressing Shift+Enter. To run the entire notebook, click on the \\\"Run\\\" button in the toolbar above the notebook cells.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>If you encounter any issues during installation or setup, please try the following:</p> <ol> <li>Check the common issues below.</li> <li>Make sure you ran <code>make env</code> to set up your environment.</li> <li>If necessary run <code>make env</code> in a fresh environment. </li> <li>Try running in a virtual environment if you skipped that step.</li> <li>Ensure you have the correct Python version (3.10 - 3.12).</li> <li>If you encounter any errors, try running <code>make test</code> to see more detailed output.</li> </ol> <p>If problems persist, please open an issue on our GitHub repository with the error message and your system information.</p>"},{"location":"troubleshooting/#common-issues","title":"Common issues","text":"<ul> <li>Running <code>make env</code> outputs <code>make: Nothing to be done for 'env'.</code></li> <li>Run <code>make refresh_env</code> (or <code>make refresh_env-no-rust</code>) to force refresh the environment.</li> <li>If you are getting <code>RuntimeError: CUDA error: no kernel image is available for execution on the device</code> or <code>UserWarning: CUDA initialization: CUDA unknown error</code>, you may be using a GPU that is incompatable with <code>vLLM</code>. See the vLLM documentation for GPU requirements.</li> <li>If you are getting <code>TypeError: log_sample() got an unexpected keyword argument 'size'</code>, you have the wrong version of <code>arsenal</code> installed. Create a fresh environment and reinstall <code>genparse</code>.</li> <li>If you are getting <code>UserWarning: Failed to initialize NumPy: _ARRAY_API not found</code> with text</li> </ul> <p>```    A module that was compiled using NumPy 1.x cannot be run in    NumPy 2.0.2 as it may crash. To support both 1.x and 2.x    versions of NumPy, modules must be compiled with NumPy 2.0.    Some module may need to rebuild instead e.g. with 'pybind11&gt;=2.12'.</p> <p>If you are a user of the module, the easiest solution will be to    downgrade to 'numpy&lt;2' or try to upgrade the affected module.    We expect that some modules will need time to support NumPy 2.    <code>``    then you should downgrade your version of numpy via</code>pip install \"numpy&lt;2\"`.</p>"},{"location":"visualizing_inference/","title":"Visualizing Inference","text":"<p>GenParse provides methods to visualize inference runs.</p> <ol> <li>Specify <code>return_record=True</code> when calling <code>InferenceSetup</code>:</li> </ol> <pre><code>result = setup(' ', n_particles=10, return_record=True)\n</code></pre> <ol> <li>Save the Sequential Monte Carlo (SMC) record of your inference run to a file by adding this to your source code:</li> </ol> <pre><code>import json\n\n# Save the SMC record to a file after the inference run.\nwith open('record.json', 'w') as f:\n      f.write(json.dumps(result.record))\n</code></pre> <p>\ud83d\udca1 Tip: You can name the file anything you like. When you run the server you will be able to navigate to the file you choose. To see an example of this code in action see the genparse_tiny_example.py.</p> <ol> <li>Run the visualization server in <code>notes/smc_viz/</code>:</li> </ol> <pre><code>python -m http.server --directory notes/smc_viz 8000\n</code></pre> <ol> <li> <p>Navigate to localhost:8000/.</p> </li> <li> <p>Select the JSON file created in step #2, then hit the Load button. </p> </li> </ol> <p>\ud83d\udca1 Tip: The visualization may not appear on the screen because it is either too small or off screen. You can zoom in and out by pulling and pinching on your track pad and can scroll the visualization by dragging with your mouse.</p> <p>The visualization will display the SMC steps, the particles at each step, and the weights of the particles. The bubbles on the left represent the weight of the particle. When particles are pruned you can see arrows splitting the continued particles to fill in the gaps.</p> <p></p>"}]}