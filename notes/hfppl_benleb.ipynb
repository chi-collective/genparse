{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dce4cbf-90c7-4b38-9e9d-048eed7edd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9851ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "if getpass.getuser() == \"benjamin.lebrun\":\n",
    "    sys.path.append(\"/home/mila/b/benjamin.lebrun/genparse\")\n",
    "    os.environ[\"HF_HOME\"] = os.path.join(os.environ[\"SCRATCH\"], \"hf_cache\")\n",
    "    print(\"HF cache set; path updated\")\n",
    "\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from random import seed\n",
    "from torch import manual_seed\n",
    "from transformers import set_seed\n",
    "\n",
    "RANDOM_SEED = 80808\n",
    "set_seed(RANDOM_SEED)\n",
    "seed(RANDOM_SEED)\n",
    "manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hfppl import Model, CachedCausalLM, LMContext\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "hfppl_llm = CachedCausalLM.from_pretrained(MODEL_ID, load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    use_fast=True,\n",
    "    prefix_token=None, \n",
    "    middle_token=None, \n",
    "    suffix_token=None, \n",
    "    eot_token=None, \n",
    "    fill_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import genparse\n",
    "from genparse.cfglm import EarleyBoolMaskCFGLM, BoolMaskCFGLM\n",
    "from genparse.util import LarkStuff\n",
    "from genparse import EOS, Float\n",
    "from arsenal.maths import sample_dict, logsumexp\n",
    "from genparse.proposal import CharacterProposal\n",
    "from genparse.lm import AsyncGreedilyTokenizedLLM\n",
    "from genparse.inference import smc_standard, smc_steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You have access to a political survey data table named \"data\", which includes the following columns:\n",
    "- \"age\" (integer)\n",
    "- \"gender\" (\"male\" or \"female\"),\n",
    "- \"year\" (integer)\n",
    "- \"state_color\" (\"blue\" or \"red\")\n",
    "- \"zipcode\" (integer)\n",
    "- \"vote\" (\"democrat\" or \"republican\") \n",
    "- \"race_ethnicity\" (\"white\", \"black\", or \"latino\").\n",
    "\n",
    "Q: Write a SQL query that shows individuals' age and gender, for people over 50 years old.\n",
    "A: SELECT age, gender FROM data WHERE age>50 {EOS}\n",
    "Q: Write a SQL query that shows individuals' vote and zipcode, ordered from lowest to highest age.\n",
    "A: SELECT vote, zipcode, age FROM data ORDER BY age ASC {EOS}\n",
    "\n",
    "Q: Write a SQL query that returns white voters' average age for each state color and sort the results.\n",
    "A:\"\"\"\n",
    "\n",
    "character_cfg = LarkStuff(\n",
    "    r\"\"\"\n",
    "        start: WS? \"SELECT\" WS select_expr WS \"FROM\" WS from_expr [WS \"WHERE\" WS bool_condition] [WS \"GROUP BY\" WS var_list] [WS \"ORDER BY\" WS orderby_expr] WS EOS\n",
    "        EOS: \"â–ª\"\n",
    "        select_expr: STAR | select_list\n",
    "        bool_condition: bool_expr | \"(\" bool_condition WS \"AND\" WS bool_condition \")\" | \"(\" bool_condition WS \"OR\" WS bool_condition \")\"\n",
    "        bool_expr: var \"=\" value | var \">\" value | var \"<\" value\n",
    "        from_expr: \"data\"\n",
    "        orderby_expr: var_list WS \"ASC\" | var_list WS \"DESC\"\n",
    "        select_list: select_var (\",\" WS select_var)*\n",
    "        var_list: var (\",\" WS var)*\n",
    "        select_var: var | \"AVG(\" var \")\" | \"MEDIAN(\" var \")\" | \"COUNT(\" var \")\"\n",
    "        var: \"age\" | \"gender\" | \"year\" | \"state_color\" | \"zipcode\" | \"vote\" | \"race_ethnicity\"\n",
    "        value: NUMBER | \"'red'\" | \"'blue'\" | \"'white'\" | \"'black'\" | \"'latino'\" | \"'republican'\" | \"'democrat'\" | \"'male'\" | \"'female'\"\n",
    "        STAR: \"*\"\n",
    "        NUMBER: /\\d+/\n",
    "        WS: /[ ]/\n",
    "\n",
    "    \"\"\"\n",
    ").char_cfg(.99, ignore='[ ]?')\n",
    "\n",
    "guide = EarleyBoolMaskCFGLM(character_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde5692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureModel(Model):\n",
    "    def __init__(self, llm, prompt, max_tokens):\n",
    "        super().__init__()\n",
    "        self.LLM = llm\n",
    "        self.context = LMContext(self.LLM, prompt)\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "    async def step(self):\n",
    "        token = await self.sample(self.context.next_token())\n",
    "\n",
    "        self.max_tokens -= 1\n",
    "\n",
    "        print(f\"{token} : {str(self.context)}\")\n",
    "        \n",
    "        # Check if done\n",
    "        if token == self.LLM.tokenizer.eos_token_id or self.max_tokens == 0:\n",
    "            self.finish()\n",
    "            return\n",
    "\n",
    "class ChracterProposalSteeringModel(Model):\n",
    "    def __init__(self, llm, guide, proposal, prompt, max_tokens, compare_time=False):\n",
    "        super().__init__()\n",
    "        self.llm = llm # AsyncGreedilyTokenizedLM\n",
    "        self.guide = guide # PCFGLM\n",
    "        self.prompt = prompt\n",
    "        self.context = []\n",
    "        self.proposal = proposal # CharacterProposal\n",
    "        self.max_tokens = max_tokens\n",
    "        self.compare_time = compare_time\n",
    "\n",
    "    async def step(self):\n",
    "        (token, llm_prob, guide_prob, proposal_prob) = await self.proposal.sample_next_token(\n",
    "            prompt=self.prompt, context=''.join(self.context), compare_time=self.compare_time\n",
    "        )\n",
    "        self.context.append(token)\n",
    "        self.weight += np.log(llm_prob) + np.log(guide_prob) - np.log(proposal_prob)\n",
    "        self.max_tokens -= 1\n",
    "\n",
    "        print(f\"`{token}` : {''.join(self.context)} : {self.weight}\")\n",
    "\n",
    "        if token == self.llm.eos or self.max_tokens == 0 or token == genparse.EOS:\n",
    "            self.finish()\n",
    "            return\n",
    "        \n",
    "    def immutable_properties(self):\n",
    "        return ['llm', 'prompt', 'guide', 'compare_token']\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"`{'' if not self.context else self.context[-1]}` : {''.join(self.context)} : {self.weight}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9933277",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 100\n",
    "BATCH_SIZE = 80\n",
    "\n",
    "hfppl_llm.batch_size = BATCH_SIZE\n",
    "genparse_llm = AsyncGreedilyTokenizedLLM(hfppl_llm, tokenizer)\n",
    "proposal = CharacterProposal(llm=genparse_llm, guide=guide)\n",
    "steering_model = ChracterProposalSteeringModel(\n",
    "    genparse_llm, guide, proposal, prompt, MAX_TOKENS, compare_time=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "particles = asyncio.run(smc_standard(steering_model, n_particles=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09008a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = Float.chart()\n",
    "for p in particles:\n",
    "    posterior[''.join(p.context)] += np.exp(p.weight)\n",
    "posterior.normalize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genparse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
