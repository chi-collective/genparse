{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to run this in your `genparse` conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF cache set; path updated\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "if getpass.getuser() == \"benjamin.lebrun\":\n",
    "    # @TIMO you may need to set this to your local genparse\n",
    "    sys.path.append(\"/home/mila/b/benjamin.lebrun/genparse\")\n",
    "    # @TIMO also set your cache IF you run into disk quota issues\n",
    "    os.environ[\"HF_HOME\"] = os.path.join(os.environ[\"SCRATCH\"], \"hf_cache\")\n",
    "    print(\"HF cache set; path updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/b/benjamin.lebrun/miniconda3/envs/genparse/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from hfppl import Model, CachedCausalLM, LMContext #, smc_standard, smc_steer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.28s/it]\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "hfppl_llm = CachedCausalLM.from_pretrained(MODEL_ID, load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    use_fast=True,\n",
    "    prefix_token=None, \n",
    "    middle_token=None, \n",
    "    suffix_token=None, \n",
    "    eot_token=None, \n",
    "    fill_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genparse\n",
    "from genparse.cfglm import EarleyBoolMaskCFGLM \n",
    "from genparse.util import LarkStuff\n",
    "from genparse import EOS, Float\n",
    "from genparse.proposal import CharacterProposal\n",
    "from genparse.inference import smc_standard, smc_steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You have access to a political survey data table named \"data\", which includes the following columns:\n",
    "- \"age\" (integer)\n",
    "- \"gender\" (\"male\" or \"female\"),\n",
    "- \"year\" (integer)\n",
    "- \"state_color\" (\"blue\" or \"red\")\n",
    "- \"zipcode\" (integer)\n",
    "- \"vote\" (\"democrat\" or \"republican\") \n",
    "- \"race_ethnicity\" (\"white\", \"black\", or \"latino\").\n",
    "\n",
    "Q: Write a SQL query that shows individuals' age and gender, for people over 50 years old.\n",
    "A: SELECT age, gender FROM data WHERE age>50 </s>\n",
    "Q: Write a SQL query that shows individuals' vote and zipcode, ordered from lowest to highest age.\n",
    "A: SELECT vote, zipcode, age FROM data ORDER BY age ASC </s>\n",
    "\n",
    "Q: Write a SQL query that returns white voters' average age for each state color.\n",
    "A:\"\"\"\n",
    "\n",
    "guide = EarleyBoolMaskCFGLM(\n",
    "    LarkStuff(\n",
    "        r\"\"\"\n",
    "            start: WS? \"SELECT\" WS select_expr WS \"FROM\" WS from_expr [WS \"WHERE\" WS bool_condition] [WS \"GROUP BY\" WS var_list] [WS \"ORDER BY\" WS orderby_expr] WS EOS\n",
    "            EOS: \"</s>\"\n",
    "            select_expr: STAR | select_list\n",
    "            bool_condition: bool_expr | \"(\" bool_condition WS \"AND\" WS bool_condition \")\" | \"(\" bool_condition WS \"OR\" WS bool_condition \")\"\n",
    "            bool_expr: var \"=\" value | var \">\" value | var \"<\" value\n",
    "            from_expr: \"data\"\n",
    "            orderby_expr: var_list WS \"ASC\" | var_list WS \"DESC\"\n",
    "            select_list: select_var (\",\" WS select_var)*\n",
    "            var_list: var (\",\" WS var)*\n",
    "            select_var: var | \"AVG(\" var \")\" | \"MEDIAN(\" var \")\" | \"COUNT(\" var \")\"\n",
    "            var: \"age\" | \"gender\" | \"year\" | \"state_color\" | \"zipcode\" | \"vote\" | \"race_ethnicity\"\n",
    "            value: NUMBER | \"'red'\" | \"'blue'\" | \"'white'\" | \"'black'\" | \"'latino'\" | \"'republican'\" | \"'democrat'\" | \"'male'\" | \"'female'\"\n",
    "            STAR: \"*\"\n",
    "            NUMBER: /\\d+/\n",
    "            WS: /[ ]/\n",
    "\n",
    "        \"\"\"\n",
    "    ).char_cfg(.99, ignore='[ ]?')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genparse import Float\n",
    "\n",
    "class SteeringModel(Model):\n",
    "    def __init__(self, llm, guide, proposal, prompt, max_tokens, compare_time=False):\n",
    "        super().__init__()\n",
    "        self.llm = llm # GreedilyTokenizedLM\n",
    "        self.guide = guide # PCFGLM\n",
    "        self.prompt = prompt\n",
    "        self.context = []\n",
    "        self.proposal = proposal # CharacterProposal\n",
    "        self.max_tokens = max_tokens\n",
    "        self.compare_time = compare_time\n",
    "\n",
    "    async def step(self):\n",
    "        (token, llm_prob, guide_prob, proposal_prob) = await self.proposal.sample_next_token(\n",
    "            context=''.join(self.context), \n",
    "            prompt=self.prompt, \n",
    "            compare_time=self.compare_time\n",
    "        )\n",
    "        self.context.append(token)\n",
    "        self.weight += np.log(llm_prob) + np.log(guide_prob) - np.log(proposal_prob)\n",
    "        self.max_tokens -= 1\n",
    "\n",
    "        print(f\"`{token}` : {''.join(self.context)} : {self.weight}\")\n",
    "\n",
    "        if token == self.llm.eos or self.max_tokens == 0 or token == genparse.EOS:\n",
    "            self.finish()\n",
    "            return\n",
    "        \n",
    "    def immutable_properties(self):\n",
    "        return ['llm', 'prompt', 'guide', 'compare_token']\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"`{'' if not self.context else self.context[-1]}` : {''.join(self.context)} : {self.weight}\"\n",
    "\n",
    "class GreedilyTokenizedLLM:\n",
    "    def __init__(self, llm, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self._model = llm # hfppl Model\n",
    "        self._decode = [self.tokenizer.decode([i]) for i in range(self.tokenizer.vocab_size)]\n",
    "        self.V = set(self._decode)\n",
    "        self.eos = self.tokenizer.eos_token\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        return self.model(self.tokenizer.encode(xs))\n",
    "\n",
    "    async def p_next(self, xs, top=None):\n",
    "        return await self._p_next(xs, top=top)\n",
    "\n",
    "    async def _p_next(self, xs, top=None):\n",
    "        assert isinstance(xs, str)\n",
    "        tokens = self.tokenizer.encode(xs)\n",
    "\n",
    "        _logp = await self._model.next_token_logprobs(tokens)\n",
    "        _p = np.exp(_logp)\n",
    "\n",
    "        if top is None:\n",
    "            top_p = _p.argsort()\n",
    "        else:\n",
    "            top_p = _p.argsort()[-top:]\n",
    "        pp = Float.chart()\n",
    "        for i in reversed(top_p):\n",
    "            pp[self._decode[i]] = _p[i]\n",
    "        if top is None:\n",
    "            return pp\n",
    "        else:\n",
    "            return pp.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 100\n",
    "BATCH_SIZE = 80\n",
    "\n",
    "hfppl_llm.batch_size = BATCH_SIZE\n",
    "genparse_llm = GreedilyTokenizedLLM(hfppl_llm, tokenizer)\n",
    "proposal = CharacterProposal(llm=genparse_llm, guide=guide)\n",
    "steering_model = SteeringModel(\n",
    "    genparse_llm, guide, proposal, prompt, MAX_TOKENS, compare_time=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "` ` :   : -1.5632851851974077\n",
      "` ` :   : -1.5632851851974077\n",
      "` ` :   : -1.5632851851974077\n",
      "` ` :   : -1.5632851851974077\n",
      "`  ` :    : 0.07733979931194535\n",
      "` ` :   : -1.5632851851974077\n",
      "` ` :   : -1.5632851851974077\n",
      "`SELECT` : SELECT : -0.717184941230105\n",
      "` ` :   : -1.5632851851974077\n",
      "`SELECT` : SELECT : -0.717184941230105\n",
      "`SELECT` :  SELECT : -3.3094512865053862\n",
      "`SELECT` :  SELECT : -3.3094512865053862\n",
      "`SELECT` :  SELECT : -3.3094512865053862\n",
      "`SELECT` :  SELECT : -3.3094512865053862\n",
      "`SELECT` :   SELECT : -2.805157440555118\n",
      "`SELECT` :  SELECT : -3.3094512865053862\n",
      "`SELECT` :  SELECT : -3.3094512865053862\n",
      "` ` : SELECT  : -4.646338486946215\n",
      "`SELECT` :  SELECT : -3.3094512865053862\n",
      "` ` : SELECT  : -4.646338486946215\n",
      "` ` :  SELECT  : -8.085529644287872\n",
      "` ` :  SELECT  : -8.085529644287872\n",
      "`  ` :  SELECT   : -5.8042791767004775\n",
      "` ` :  SELECT  : -8.085529644287872\n",
      "` ` :   SELECT  : -6.955299892692902\n",
      "` ` :  SELECT  : -8.085529644287872\n",
      "` ` :  SELECT  : -8.085529644287872\n",
      "`race` : SELECT race : -5.470040360630256\n",
      "` ` :  SELECT  : -8.085529644287872\n",
      "`age` : SELECT age : -11.95497796097943\n",
      "`age` :  SELECT age : -13.492562459012198\n",
      "`age` :  SELECT  age : -14.332043633805425\n",
      "`_` : SELECT race_ : -11.558458098788403\n",
      "`_` : SELECT race_ : -11.558458098788403\n",
      "`age` :  SELECT  age : -14.332043633805425\n",
      "`_` : SELECT race_ : -11.558458098788403\n",
      "`vot` :  SELECT  vot : -9.370626622179852\n",
      "`vot` :  SELECT  vot : -9.370626622179852\n",
      "`_` : SELECT race_ : -11.558458098788403\n",
      "`vot` :  SELECT  vot : -9.370626622179852\n",
      "`e` :  SELECT  vote : -10.829288796035652\n",
      "`e` :  SELECT  vote : -10.829288796035652\n",
      "`e` :  SELECT  vote : -10.829288796035652\n",
      "`e` :  SELECT  vote : -10.829288796035652\n",
      "`e` :  SELECT  vote : -10.829288796035652\n",
      "`eth` : SELECT race_eth : -11.497656977731484\n",
      "`e` :  SELECT  vote : -10.829288796035652\n",
      "`eth` : SELECT race_eth : -11.497656977731484\n",
      "`e` :  SELECT  vote : -10.829288796035652\n",
      "`e` :  SELECT  vote : -10.829288796035652\n",
      "`,` :  SELECT  vote, : -11.189643803215231\n",
      "`,` :  SELECT  vote, : -11.189643803215231\n",
      "`,` :  SELECT  vote, : -11.189643803215231\n",
      "`,` :  SELECT  vote, : -11.189643803215231\n",
      "`,` :  SELECT  vote, : -11.189643803215231\n",
      "`nic` : SELECT race_ethnic : -15.86374228349387\n",
      "`,` :  SELECT  vote, : -11.189643803215231\n",
      "`nic` : SELECT race_ethnic : -15.86374228349387\n",
      "`,` :  SELECT  vote, : -11.189643803215231\n",
      "`,` :  SELECT  vote, : -11.189643803215231\n"
     ]
    }
   ],
   "source": [
    "particles = asyncio.run(\n",
    "    smc_standard(steering_model, n_particles=10)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
