{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce4cbf-90c7-4b38-9e9d-048eed7edd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae7240-4f92-4c89-8997-ba5263a819a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import asyncio\n",
    "import os\n",
    "import copy\n",
    "from time import time\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e6c90-45fd-4648-b13c-a38ffff07efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hfppl import Model, CachedCausalLM, LMContext, smc_standard\n",
    "from hfppl.distributions import TokenCategorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498a3d5-9710-4c8f-a635-2e582ba5c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the language model. \n",
    "# Mistral and Vicuna are open models; to use a model with restricted access, like LLaMA 2,\n",
    "# pass your HuggingFace API key as the optional `auth_token` argument:\n",
    "# LLM = CachedCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", auth_token=os.environ['HF_AUTH_TOKEN'])\n",
    "# LLM = CachedCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "# LLM = CachedCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "MODEL_ID = \"codellama/CodeLlama-7b-Instruct-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260ca34-1fc0-4e25-bba4-88fcdfe5065a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timv/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cb55d00fd84f5d9f5aa08020387d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LLM = CachedCausalLM.from_pretrained(MODEL_ID, load_in_8bit=True)\n",
    "#LLM.batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c628c-cfcf-4f35-adbd-5041b92fad71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc8ecd-6eb5-4e5f-a44f-3de26d4d8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, prefix_token=None, middle_token=None, suffix_token=None, eot_token=None, fill_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7653e826-bec0-44c6-a780-57c85daffec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You have access to a political survey data table named \"data\", which includes the following columns:\n",
    "- \"age\" (integer)\n",
    "- \"gender\" (\"male\" or \"female\"),\n",
    "- \"year\" (integer)\n",
    "- \"state_color\" (\"blue\" or \"red\")\n",
    "- \"zipcode\" (integer)\n",
    "- \"vote\" (\"democrat\" or \"republican\") \n",
    "- \"race_ethnicity\" (\"white\", \"black\", or \"latino\").\n",
    "\n",
    "Q: Write a SQL query that shows individuals' age and gender, for people over 50 years old.\n",
    "A: SELECT age, gender FROM data WHERE age>50 </s>\n",
    "Q: Write a SQL query that shows individuals' vote and zipcode, ordered from lowest to highest age.\n",
    "A: SELECT vote, zipcode, age FROM data ORDER BY age ASC </s>\n",
    "Q: Write a SQL query that returns white voters' average age for each state color.\n",
    "A:\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbded5d-d072-4cdb-bdab-f1dfc5fe2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM.cache_kv(LLM.tokenizer.encode(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771027e-1d4a-461c-a890-ed1dbfb8cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = prompt+\" SELECT state_color\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b78db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genparse import Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedilyTokenizedLLM:\n",
    "\n",
    "    def __init__(self, llm, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self._model = llm\n",
    "\n",
    "        self._decode = [self.tokenizer.decode([i]) for i in range(self.tokenizer.vocab_size)]\n",
    "        self.V = set(self._decode)\n",
    "        self.eos = self.tokenizer.eos_token\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        return self.model(self.tokenizer.encode(xs))\n",
    "\n",
    "    def p_next(self, xs, top=None):\n",
    "        return asyncio.run(self._p_next(xs, top=top))\n",
    "\n",
    "    async def _p_next(self, xs, top=None):\n",
    "        assert isinstance(xs, str)\n",
    "        tokens = self.tokenizer.encode(xs)\n",
    "\n",
    "        _logp = await self._model.next_token_logprobs(tokens)\n",
    "        _p = np.exp(_logp)\n",
    "\n",
    "        if top is None:\n",
    "            top_p = _p.argsort()\n",
    "        else:\n",
    "            top_p = _p.argsort()[-top:]\n",
    "        pp = Float.chart()\n",
    "        for i in reversed(top_p):\n",
    "            pp[self._decode[i]] = _p[i]\n",
    "        if top is None:\n",
    "            return pp\n",
    "        else:\n",
    "            return pp.normalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0afa825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genparse.experimental.earley import Earley\n",
    "from genparse.lm import LM\n",
    "from genparse import Boolean, EOS, add_EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eec6d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarleyBoolMaskCFGLM(LM):\n",
    "    \"LM-like interface for Boolean-masking CFG models.\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        if EOS not in cfg.V: cfg = add_EOS(cfg)\n",
    "        if cfg.R != Boolean: cfg = cfg.map_values(lambda x: Boolean(x>0), Boolean)\n",
    "        self.model = Earley(cfg.prefix_grammar.renumber().nullaryremove().unarycycleremove().renumber())\n",
    "        super().__init__(eos = self.model.eos, V = self.model.V)\n",
    "\n",
    "    def p_next(self, context):\n",
    "        p = self.model.p_next(context).trim()\n",
    "        return Float.chart({w: 1 for w in p})\n",
    "\n",
    "    def __call__(self, context):\n",
    "        return float(self.model(context) != Boolean.zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f6e81ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GreedilyTokenizedLLM(LLM, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c62ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Monospace;\"><table><tr style=\"font-weight: bold;\"><td>key</td><td>value</td></tr><tr><td><pre>,</pre></td><td><pre>0.069331209816617</pre></td> </tr><tr><td><pre>AS</pre></td><td><pre>0.668154972391995</pre></td> </tr><tr><td><pre>as</pre></td><td><pre>0.19444419189443085</pre></td> </tr><tr><td><pre>.</pre></td><td><pre>0.021477806567383823</pre></td> </tr><tr><td><pre>\n",
       "</pre></td><td><pre>0.014532619249118384</pre></td> </tr><tr><td><pre>_</pre></td><td><pre>0.010224974054857044</pre></td> </tr><tr><td><pre>&quot;</pre></td><td><pre>0.007658152378803844</pre></td> </tr><tr><td><pre>,&quot;</pre></td><td><pre>0.007364772352179361</pre></td> </tr><tr><td><pre>state</pre></td><td><pre>0.006811301294614755</pre></td> </tr></table></div>"
      ],
      "text/plain": [
       "{',': 0.069331209816617, 'AS': 0.668154972391995, 'as': 0.19444419189443085, '.': 0.021477806567383823, '\\n': 0.014532619249118384, '_': 0.010224974054857044, '\"': 0.007658152378803844, ',\"': 0.007364772352179361, 'state': 0.006811301294614755}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.p_next(context, top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e05cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genparse.proposal import CharacterProposal, TokenProposal\n",
    "from genparse.cfglm import BoolMaskCFGLM, locally_normalize\n",
    "from genparse.util import LarkStuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8fd3b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = EarleyBoolMaskCFGLM(LarkStuff(r\"\"\"\n",
    "start: WS? \"SELECT\" WS select_expr WS \"FROM\" WS from_expr [WS \"WHERE\" WS bool_condition] [WS \"GROUP BY\" WS var_list] [WS \"ORDER BY\" WS orderby_expr] WS EOS\n",
    "EOS: \"</s>\"\n",
    "select_expr: STAR | select_list\n",
    "bool_condition: bool_expr | \"(\" bool_condition WS \"AND\" WS bool_condition \")\" | \"(\" bool_condition WS \"OR\" WS bool_condition \")\"\n",
    "bool_expr: var \"=\" value | var \">\" value | var \"<\" value\n",
    "from_expr: \"data\"\n",
    "orderby_expr: var_list WS \"ASC\" | var_list WS \"DESC\"\n",
    "select_list: select_var (\",\" WS select_var)*\n",
    "var_list: var (\",\" WS var)*\n",
    "select_var: var | \"AVG(\" var \")\" | \"MEDIAN(\" var \")\" | \"COUNT(\" var \")\"\n",
    "var: \"age\" | \"gender\" | \"year\" | \"state_color\" | \"zipcode\" | \"vote\" | \"race_ethnicity\"\n",
    "value: NUMBER | \"'red'\" | \"'blue'\" | \"'white'\" | \"'black'\" | \"'latino'\" | \"'republican'\" | \"'democrat'\" | \"'male'\" | \"'female'\"\n",
    "STAR: \"*\"\n",
    "NUMBER: /\\d+/\n",
    "WS: /[ \\n]/\n",
    "\n",
    "\"\"\").char_cfg(.99, ignore='[ ]?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df701999",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal = CharacterProposal(llm=m, guide=guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3af1affe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mSELECT\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mage\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mFROM\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mdata\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mWHERE\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mvote\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m='\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mwhite\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m'\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mGROUP\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mBY\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mstate\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m_\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mcolor\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mORDER\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mBY\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mstate\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m_\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mcolor\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mASC\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m</\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36ms\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m>\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\n",
      "guided-sample is 3.2075x faster than llm (\u001b[0;33mp=0.07926\u001b[0m, median: llm: 0.285727, guided-sample: 0.0890797)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"\\n SELECT\\n age\\nFROM\\ndata\\nWHERE\\nvote ='white'\\nGROUP BY\\nstate_color\\n ORDER BY\\nstate_color\\nASC\\n </s> \",\n",
       " 6.841781492459149e-10)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposal.sample(prompt, verbosity=1, max_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac76e80",
   "metadata": {},
   "source": [
    "Q: Write a SQL query that returns white voters' average age for each state color.\n",
    "\n",
    "A: SELECT state_color, AVG(age) FROM data WHERE race_ethnicity = 'white' GROUP BY state_color `</s>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bfaa6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_proposal = TokenProposal(llm=m, guide=guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mSELECT\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mvot\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36me\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mFROM\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mdata\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mWHERE\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mstate\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m_\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mcolor\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m='\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mblue\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m'\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mGROUP\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mBY\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mstate\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m_\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mcolo\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mr\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mORDER\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mBY\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mage\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36mDESC\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m\n",
      "\u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m \u001b[0m\u001b[0;35m|\u001b[0m\u001b[0;36m</\u001b[0m\u001b[0;35m|\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\" SELECT\\n vote\\nFROM\\ndata\\nWHERE\\nstate_color='blue'\\nGROUP BY\\nstate_color\\n ORDER BY\\nage\\nDESC\\n </\",\n",
       " 0.0)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_proposal.sample(prompt, verbosity=1, max_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42c3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guide(\"\"\"SELECT state_color, AVG(age) FROM data WHERE race_ethnicity = 'white' GROUP BY state_color </s>\"\"\" + guide.eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b6e55-af5c-4375-a543-c993c29d048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureModel(Model):\n",
    "    def __init__(self, prompt, max_tokens):\n",
    "        super().__init__()\n",
    "        self.context = LMContext(LLM, prompt)\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "    async def step(self):\n",
    "        logprobs = self.context.next_token_logprobs.copy()\n",
    "        proposal = TokenCategorical(LLM, logprobs)     \n",
    "        \n",
    "        token = await self.sample(self.context.next_token(), proposal=proposal)\n",
    "        self.score(logprobs[token.token_id])\n",
    "        self.max_tokens -= 1\n",
    "        \n",
    "        # Check if done\n",
    "        if token == LLM.tokenizer.eos_token_id or self.max_tokens == 0:\n",
    "            self.finish()\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf89bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481f429-b311-45b3-bd15-3b6349305e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    constraint_model = PureModel(prompt, 50)\n",
    "    particles = await smc_standard(constraint_model, 10)\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99c6b61-301a-49b5-b8e6-b361cb8f8364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a7f6e-b9f9-4ac1-a240-ecf416ba8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = time()\n",
    "particles = await main()\n",
    "took = time() - before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b49ce-be69-44f1-9ce9-284876061edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.49 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "print(f'{sum(len(p.context.tokens) for p in particles)/took:.2f} tokens/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e236fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.859547066737412e-13 \" SELECT state_color, AVG(age) FROM data WHERE race_ethnicity = 'white' GROUP BY state_color\\n\\nQ: Write a SQL query that returns the state color and average zipcode for democrat votes.\\n\"\n",
      "3.97415483928935e-09 \" SELECT state_color, AVG(age) FROM data WHERE race_ethnicity = 'white' GROUP BY state_color\\n\\nQ: Write a SQL query that returns the state color with most white voters.\\nA: SELECT\"\n",
      "3.97415483928935e-09 \" SELECT state_color, AVG(age) FROM data WHERE race_ethnicity = 'white' GROUP BY state_color\\n\\nQ: Write a SQL query that returns the state color with most white voters.\\nA: SELECT\"\n",
      "4.1457076307306947e-10 \" SELECT state_color, AVG(age) FROM data WHERE race_ethnicity = 'white' GROUP BY state_color\\n\\nQ: Write a SQL query that returns the state color and the number of red voters in each state\"\n",
      "3.97415483928935e-09 \" SELECT state_color, AVG(age) FROM data WHERE race_ethnicity = 'white' GROUP BY state_color\\n\\nQ: Write a SQL query that returns the state color with most white voters.\\nA: SELECT\"\n",
      "6.289751909264641e-13 \" SELECT state_color, AVG(age) FROM data WHERE race_ethnicity = 'white' GROUP BY state_color\\n\\nQ: Write a SQL query that returns the state color with most white voters, ordered by the average\"\n",
      "3.97415483928935e-09 \" SELECT state_color, AVG(age) FROM data WHERE race_ethnicity = 'white' GROUP BY state_color\\n\\nQ: Write a SQL query that returns the state color with most white voters.\\nA: SELECT\"\n",
      "3.97415483928935e-09 \" SELECT state_color, AVG(age) FROM data WHERE race_ethnicity = 'white' GROUP BY state_color\\n\\nQ: Write a SQL query that returns the state color with most white voters.\\nA: SELECT\"\n",
      "6.529514697384484e-11 \" SELECT state_color, AVG(age) FROM data WHERE race_ethnicity = 'white' GROUP BY state_color\\n\\nQ: Write a SQL query that returns the state color with most white voters for each state color.\"\n",
      "3.97415483928935e-09 \" SELECT state_color, AVG(age) FROM data WHERE race_ethnicity = 'white' GROUP BY state_color\\n\\nQ: Write a SQL query that returns the state color with most white voters.\\nA: SELECT\"\n"
     ]
    }
   ],
   "source": [
    "for p in particles:\n",
    "    print(np.exp(p.weight), repr(str(p.context)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133819e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fade5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_tokens = [LLM.tokenizer.decode([i]) for i in range(len(LLM.tokenizer.vocab))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
