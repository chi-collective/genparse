{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dce2b33-6852-4f53-9e91-22ecedb2f381",
   "metadata": {},
   "source": [
    "# Sentence to PClean query demo\n",
    "This notebook demos the sentence-to-PClean pipeline. Given a sentence giving information about some doctor, it generates and runs queries about those people against the Medicare dataset used in Alex Lew's paper using a PClean model trained by Ian Limarta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ae16ed-e3b5-441d-8f0a-5d78b9fb835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import string\n",
    "from typing import Any, Optional, TypeVar, Union\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import lark\n",
    "import nest_asyncio\n",
    "import requests\n",
    "import spacy\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import vllm\n",
    "\n",
    "import genparse\n",
    "from genparse import InferenceSetup, InferenceSetupVLLM\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "logger = logging.getLogger(\"notebook\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "088033b5-c175-43e6-9332-5ffa2aa9cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_sentences_path = Path().resolve() / \"debug_sentences.jsonl\"\n",
    "debug_sentences = [json.loads(line) for line in debug_sentences_path.read_text(encoding=\"utf-8\").splitlines() if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e71ec4e-1a6a-4c65-9192-eaf10cc559fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_debug_sentences_path = Path().resolve() / \"debug_sentences_detailed.jsonl\"\n",
    "detailed_debug_sentences = [json.loads(line) for line in detailed_debug_sentences_path.read_text(encoding=\"utf-8\").splitlines() if line.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd6668f-bba0-4fff-96dd-2dbd6355c851",
   "metadata": {},
   "source": [
    "## Example inputs and outputs\n",
    "\n",
    "Taken from the first ten sentences (rows 2-11) in the [GPT-4 tweets Marjorie Freedman generated][gpt4_tweets].\n",
    "\n",
    "[gpt4_tweets]: https://docs.google.com/spreadsheets/d/1vq_HAdbFY079vYVOppkFLk_zojJ46MJKsy-TzUm7Q3M/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c079f1-91a3-4c2b-a075-56c460f12d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Oops, one of these examples is missing a key... `'example_output'`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    sentence_table_row_template = string.Template(\"| $sentence | <pre>$example_output</pre> |\")\n",
    "    def show_sentences(sentence_data):\n",
    "        lines = [\"| Sentence | Example PClean preamble code |\", \"| --- | --- |\"]\n",
    "        for sentence_datum in sentence_data:\n",
    "            lines.append(sentence_table_row_template.substitute(sentence=sentence_datum[\"sentence\"], example_output=sentence_datum[\"example_output\"].replace(\"\\n\", \"<br>\")))\n",
    "        markdown = \"\\n\".join(lines)\n",
    "        display(Markdown(markdown))\n",
    "    \n",
    "    \n",
    "    show_sentences(debug_sentences)\n",
    "except KeyError as e:\n",
    "    display(Markdown(f'Oops, one of these examples is missing a key... `{e}`'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03784340-c67a-485c-9624-77ef5936d5e7",
   "metadata": {},
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bffc6b-ec42-4b0c-8a92-9f68830032eb",
   "metadata": {},
   "source": [
    "### SpaCy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad06c20-4e09-4ca2-84af-285dd23c3dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just had an insightful consultation with Dr. Kay Ryan in Baltimore. Feeling optimistic about my health! #HealthMatters #Baltimore\n",
      "(Kay Ryan, Baltimore, Baltimore)\n",
      "\n",
      "Dr. Kay Ryan's office at 321 Pine St. in Baltimore is so welcoming and efficient. Highly recommend! #CityCare\n",
      "(Kay Ryan, Baltimore, CityCare)\n",
      "\n",
      "Exploring Baltimore after an informative appointment with Dr. Kay Ryan. Loving this city's vibe! #BaltimoreAdventures #DoctorVisit\n",
      "(Baltimore, Kay Ryan)\n",
      "\n",
      "Dr. Kay Ryan, a neurologist, provided excellent care today. Grateful for her expertise! #Healthcare #BaltimoreDoctors\n",
      "(Kay Ryan, today)\n",
      "\n",
      "Feeling reassured after my visit to Dr. Kay Ryan in Baltimore. She's truly exceptional! #FeelingGood #CityOfCharm\n",
      "(Kay Ryan, Baltimore)\n",
      "\n",
      "Had a great experience with Dr. Kay Ryan at 321 Pine St. in Baltimore. Her attention to detail is unmatched! #HealthCheck\n",
      "(Kay Ryan, Baltimore)\n",
      "\n",
      "From diagnosis to treatment, Dr. Kay Ryan, a cardiologist, covered it all. Baltimore, you're lucky to have her! #MedicalCare\n",
      "(Kay Ryan, Baltimore)\n",
      "\n",
      "Just wrapped up my appointment with Dr. Kay Ryan in Baltimore. Her professionalism is top-notch. #HealthJourney\n",
      "(Kay Ryan, Baltimore)\n",
      "\n",
      "Dr. Kay Ryan's office in Baltimore is so efficient and welcoming. A great experience overall! #DoctorVisit #CityCare\n",
      "(Kay Ryan, Baltimore)\n",
      "\n",
      "Can't say enough good things about Dr. Kay Ryan, a pediatrician, in Baltimore. Truly an outstanding doctor! #Grateful\n",
      "(Kay Ryan, Baltimore)\n",
      "\n",
      "Loved visiting Dr. Kilmore's obstetrics practice in Camp Hill! Can't say enough good things.\n",
      "(Kilmore, Camp Hill)\n",
      "\n",
      "Dr. Kilmore in Boston saved my life with his incredible cardiology skills. Forever grateful! #BostonDocs #HeartHealth\n",
      "(Kilmore, Boston)\n",
      "\n",
      "Just had a life-changing neurology consult with Dr. Faraday in Seattle. Truly a genius! #SeattleScience #NeuroExperts\n",
      "(Faraday, Seattle)\n",
      "\n",
      "My skin has never been better thanks to Dr. Linwood in Denver. Highly recommend! #DenverDerm #SkinCare\n",
      "(Linwood, Denver)\n",
      "\n",
      "Dr. Voss in Miami was amazing with my child’s treatment. Pediatric care at its finest! #MiamiMeds #KidsHealth\n",
      "(Voss, Miami)\n",
      "\n",
      "Dr. Ravenswood in San Francisco fixed my knee like a pro. Orthopedic wizard! #SFOOrtho #BoneHealth\n",
      "(Ravenswood, San Francisco)\n",
      "\n",
      "Grateful for Dr. Shapiro in Chicago who guided me through my cancer journey. The best oncologist! #ChicagoCare #FightCancer\n",
      "(Shapiro, Chicago)\n",
      "\n",
      "Feeling mentally strong thanks to Dr. Everhart in NYC. Exceptional psychiatrist! #NYCWellness #MentalHealth\n",
      "(Everhart, NYC)\n",
      "\n",
      "Dr. Jamison in Dallas balanced my hormones perfectly. Best endocrinologist ever! #DallasDocs #HormoneHealth\n",
      "(Jamison, Dallas)\n",
      "\n",
      "Seeing clearly again thanks to Dr. Thornton in LA. Incredible ophthalmologist! #LAVisions #EyeCare\n",
      "(Thornton, LA)\n",
      "\n",
      "Dr. Garrison in Phoenix fixed my indigestion and it only took one visit. Best gastroenterologist around! #PhoenixHealth #DigestiveCare\n",
      "(Garrison, Phoenix, one)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Supporting code for entity extraction.\n",
    "_SPACY_MODEL_NAME = 'en_core_web_trf'\n",
    "spacy_model = spacy.load(_SPACY_MODEL_NAME)\n",
    "_PERSON_LABEL = 'PERSON'\n",
    "_LOCATION_LABEL = 'LOC'\n",
    "# spaCy labels list example thanks to Stack Overflow user 'russhoppa': https://stackoverflow.com/a/78252807\n",
    "_SPACY_LABELS = spacy_model.get_pipe(\"ner\").labels\n",
    "assert _PERSON_LABEL in _SPACY_LABELS\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "def _uniquify(items: list[T]) -> list[T]:\n",
    "    \"\"\"\n",
    "    O(n^2) order-preserving uniquification.\n",
    "\n",
    "    Fine for short lists like a single sentence's list of PERSON entities.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for item in items:\n",
    "        if item not in result:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "def get_people(sentence: str) -> list[str]:\n",
    "    return _uniquify(\n",
    "        [ent.text for ent in spacy_model(sentence).ents if ent.label_ == _PERSON_LABEL]\n",
    "    )\n",
    "\n",
    "def get_locations(sentence: str) -> list[str]:\n",
    "    return _uniquify(\n",
    "        [ent.text for ent in spacy_model(sentence).ents if ent.label_ == _LOCATION_LABEL]\n",
    "    )\n",
    "\n",
    "def show_ents(sentence: str) -> None:\n",
    "    print(sentence)\n",
    "    print(spacy_model(sentence).ents)\n",
    "\n",
    "for sentence_datum in debug_sentences:\n",
    "    show_ents(sentence_datum[\"sentence\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6069e-d97c-4b8f-8697-62a9148bdfac",
   "metadata": {},
   "source": [
    "### Extracting the generated code from a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5444b2ba-605d-45cf-b7c5-57073c132990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_from_response(text: str) -> str:\n",
    "    result = text.removeprefix('<|start_header_id|>assistant<|end_header_id|>').strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b9dc8-ce6b-413b-a32a-6f58f05c3a15",
   "metadata": {},
   "source": [
    "### Sort posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b87cf18-b455-4de3-8aae-75d3ec3a95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_posterior(posterior):\n",
    "    return {inference: likelihood for inference, likelihood in sorted(posterior.items(), key=lambda t: (t[1], t[0]), reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c8f11-660c-4c9e-9918-3a36cbf3535a",
   "metadata": {},
   "source": [
    "### Aggregate likelihoods over extracted code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65a7792-3619-4df0-9128-ed85c41dcc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_likelihoods(posterior: dict[str, float]) -> dict[str, float]:\n",
    "    result = {}\n",
    "    for inference, likelihood in posterior.items():\n",
    "        code_only = extract_code_from_response(inference)\n",
    "        try:\n",
    "            code_only = json.dumps(\n",
    "                {key: value for key, value in sorted(json.loads(code_only).items(), key=lambda t: t[0])},\n",
    "                indent=2\n",
    "            )\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        result.setdefault(code_only, 0.0)\n",
    "        result[code_only] += likelihood\n",
    "    return sort_posterior(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b68b56-db12-4264-a14b-de533be8e5c5",
   "metadata": {},
   "source": [
    "### Get best inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a463c9d-5e48-4bdb-b6d8-0e9f8ea069ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_inference(posterior: dict[str, float]) -> tuple[str, float]:\n",
    "    return max(posterior.items(), key=lambda t: (t[1], t[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67b831-8d2e-401d-9523-6f9602c74151",
   "metadata": {},
   "source": [
    "### Running inference locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a725710-f405-4b3a-a101-93b3a3256e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER_MODEL_ID = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "SERVER_GENPARSE_MODEL_NAME = \"llama3.1\"\n",
    "_LOCAL_BATCH_SIZE = 1\n",
    "\n",
    "class FakeResponse:\n",
    "    def __init__(self, data):\n",
    "        self._data = data\n",
    "    \n",
    "    @property\n",
    "    def status_code(self):\n",
    "        return 200\n",
    "\n",
    "    def json(self):\n",
    "        return self._data\n",
    "\n",
    "    def text(self):\n",
    "        return json.dumps(self._data)\n",
    "\n",
    "def server_model(grammar, proposal=\"character\"):\n",
    "    return InferenceSetupVLLM(SERVER_GENPARSE_MODEL_NAME, grammar, proposal_name=proposal)\n",
    "\n",
    "def run_inference_vllm(\n",
    "    prompt: str,\n",
    "    *,\n",
    "    proposal: str = 'character',\n",
    "    batch_size: int = _LOCAL_BATCH_SIZE,\n",
    "    max_tokens: int,\n",
    "    n_particles: int,\n",
    "    temperature: float = 1.0,\n",
    "    grammar: str,\n",
    "    genparse_url: str = \"\",\n",
    "    inference_setup: InferenceSetupVLLM,\n",
    ") -> requests.Response:\n",
    "    #assert batch_size == inference_setup.sampler.llm._model.batch_size\n",
    "    assert grammar == inference_setup.sampler.grammar\n",
    "    assert (proposal == 'character' and isinstance(inference_setup, genparse.CharacterProposal))\n",
    "    inference = inference_setup(prompt, n_particles=n_particles, temperature=temperature, max_tokens=max_tokens)\n",
    "    return FakeResponse({\"log_ml\": None, \"posterior\": inference.posterior})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f54459-5000-44c3-90a7-01c817e87b29",
   "metadata": {},
   "source": [
    "### Running the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "423fe5b1-ad77-46bb-8d3e-1bc026e0d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_BATCH_SIZE = 1\n",
    "# Ben LeBrun's WIP server running on GCP as of 2024-07-16\n",
    "_DEFAULT_GENPARSE_INFERENCE_SERVER_RESTART_URI = 'http://34.122.30.137:9999/restart'\n",
    "_DEFAULT_GENPARSE_INFERENCE_SERVER_URI = 'http://34.122.30.137:8888/infer'\n",
    "#_DEFAULT_GENPARSE_INFERENCE_SERVER_RESTART_URI = 'http://34.70.201.1:9999/restart'\n",
    "#_DEFAULT_GENPARSE_INFERENCE_SERVER_URI = 'http://34.70.201.1:8888/infer'\n",
    "\n",
    "def restart_inference_server():\n",
    "    basic = requests.auth.HTTPBasicAuth(input(\"User: \"), input('Password: '))\n",
    "    result = requests.post(_DEFAULT_GENPARSE_INFERENCE_SERVER_RESTART_URI, auth=basic)\n",
    "    return result\n",
    "\n",
    "def run_inference_server(\n",
    "    prompt: str,\n",
    "    *,\n",
    "    proposal: str = 'character',\n",
    "    batch_size: int = _BATCH_SIZE,\n",
    "    max_tokens: int,\n",
    "    n_particles: int,\n",
    "    temperature: float = 1.0,\n",
    "    grammar: str,\n",
    "    genparse_url: str = _DEFAULT_GENPARSE_INFERENCE_SERVER_URI,\n",
    ") -> requests.Response:\n",
    "    \"\"\"\n",
    "    Run inference using a server.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'prompt': prompt,\n",
    "        'method': 'smc-standard',\n",
    "        'n_particles': n_particles,\n",
    "        'lark_grammar': grammar,\n",
    "        'proposal_name': proposal,\n",
    "        'proposal_args': {},\n",
    "        'max_tokens': max_tokens,\n",
    "        'temperature': temperature,\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    response = requests.post(genparse_url, json=params, headers=headers)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3b15b36-564c-4dcf-bbc5-5ec3c8ab8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#restart_inference_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec71b2d-9800-4ab0-b9cd-922fa623e47c",
   "metadata": {},
   "source": [
    "### JSON generation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4555ae6e-7c54-410a-9ee2-de168caf6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_prompt = string.Template(\n",
    "    \"\"\"Write a flat JSON object describing one of the doctors in the given input sentence.\n",
    "\n",
    "In general, your output should look like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    “some_feature”: “foo”,\n",
    "    “another_feature”: “bar”,\n",
    "    ...\n",
    "    “last_feature”: “baz”\n",
    "}\n",
    "```\n",
    "\n",
    "There are eight features we would like to extract from the sentence:\n",
    "\n",
    "- “first” (the doctor’s given name)\n",
    "- “last” (the doctor’s surname)\n",
    "- “specialty” (the doctor’s specialty)\n",
    "- “legal_name” (the legal name of the doctor’s business, practice, or employer)\n",
    "- “city_name” (the city where the doctor’s office is)\n",
    "- “zip” (the ZIP code of the doctor’s office)\n",
    "- “addr” (the doctor’s office address, “line 1\" -- this is the street address)\n",
    "- “addr2” (additional address information giving a suite number, unit number, et cetera -- “line 2\" of the address)\n",
    "\n",
    "Please generate a key-value pair for every such value specified in the sentence. Avoid nulls and empty strings. Omit any missing values. If the sentence calls the doctor “Dr. John Smith,” include both the given and surname (`“first”` and `“last”`). If the sentence calls the doctor “Dr. Smith,” include only the surname (`“last”`). Do not write a script. Output the JSON itself directly with no preamble or commentary. The following examples illustrate how you should behave on the input sentence.\n",
    "\n",
    "Input: What a great experience with Dr. Williams’s pediatrics office in Victoria! My child has never been happier to be jabbed with a needle. #ParentingVictories #Victoria\n",
    "Output: ```json\n",
    "{“last”: “Williams”, “specialty”: “pediatrics”, “city_name”: “Victoria”}\n",
    "```\n",
    "\n",
    "Input: John Smith’s neurology office (Happy Brain Services LLC) at 512 Example Street Suite 3600 (Camp Hill) is terrible!\n",
    "Output: ```json\n",
    "{“first”: “John”, “last”: “Smith”, “specialty”: “neurology”, “legal_name”: “Happy Brain Services LLC”, “addr”: “512 Example Street”, “addr2\": “Suite 3600”, “city_name”: “Camp Hill”}\n",
    "```\n",
    "\n",
    "Input: Loved visiting Dr. Kay Ryan’s neurology office (Green Medicine Inc.) at 256 Overflow St (ZIP 17011-2202)! No wait time at all. #CampHill\n",
    "Output: ```json\n",
    "{“first”: “Kay”, “last”: “Ryan”, “legal_name”: “Green Medicine Inc”, “specialty”: “neurology”, “addr”: “256 Overflow St”, “zip”: “170112202”, “city_name”: “Camp Hill”}\n",
    "```\n",
    "\n",
    "Input: Dr. Pat Rogers’s office screwed us! So much for Soulful Medical Services Inc. Took our money and Pat gave us three minutes tops. #BaltimoreSucks\n",
    "Output: ```json\n",
    "{“first”: “Pat”, “last”: “Rogers”, “legal_name”: “Soulful Medical Services Inc”, “city_name”: “Baltimore”}\n",
    "```\n",
    "\n",
    "Input: Dr. Maynard’s really rules! They figured out my illness when no one else could. Loving the care here in #Lexington\n",
    "Output: ```json\n",
    "{“last”: “Maynard”, “city_name”: “Lexington”}\n",
    "```\n",
    "\n",
    "The following is your input sentence. Produce the appropriate output.\n",
    "\n",
    "Input: $sentence\n",
    "Output:\"\"\"\n",
    ")\n",
    "# scrapped\n",
    "# The doctor's name is $name. Their office address is $address. Their specialty is $specialty. The city is $city.\n",
    "\n",
    "def format_json_prompt(sentence: str) -> str:\n",
    "    return json_prompt.substitute(sentence=sentence)\n",
    "\n",
    "print(json_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868985bf-9143-4cbd-9eb3-f22388b9454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_prompt2 = string.Template(\n",
    "    \"\"\"Write a flat JSON object describing one of the doctors in the given input sentence.\n",
    "\n",
    "In general, your output should look like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"some_feature\": \"foo\",\n",
    "    \"another_feature\": \"bar\",\n",
    "    ...\n",
    "    \"last_feature\": \"baz\"\n",
    "}\n",
    "```\n",
    "\n",
    "There are eight features we would like to extract from the sentence:\n",
    "\n",
    "- \"first\" (the doctor's given name)\n",
    "- \"last\" (the doctor's surname)\n",
    "- \"specialty\" (the doctor's specialty)\n",
    "- \"legal_name\" (the legal name of the doctor's business, practice, or employer)\n",
    "- \"city_name\" (the city where the doctor's office is)\n",
    "- \"zip\" (the ZIP code of the doctor's office)\n",
    "- \"addr\" (the doctor's office address, \"line 1\" -- this is the street address)\n",
    "- \"addr2\" (additional address information giving a suite number, unit number, et cetera -- \"line 2\" of the address)\n",
    "\n",
    "Please generate for each feature a key-value pair corresponding to the information given in the sentence. If the sentence doesn't specify a feature, simply output `null`. If the sentence calls the doctor \"Dr. John Smith,\" include both the given name (`\"first\"`) and the surname (`\"last\"`). If the sentence calls the doctor \"Dr. Smith,\" assume it is the surname and include only that surname (as `\"last\"`). Do not write a script. Output the JSON itself directly with no preamble or commentary. The following examples illustrate how you should behave on the input sentence.\n",
    "\n",
    "Input: What a great experience with Dr. Williams's pediatrics office in Victoria! My child has never been happier to be jabbed with a needle. #ParentingVictories #Victoria\n",
    "Output: ```json\n",
    "{\"first\": null, \"last\": \"Williams\", \"specialty\": \"pediatrics\", \"legal_name\": null, \"city_name\": \"Victoria\", \"zip\": null, \"addr\": null, \"addr2\": null}\n",
    "```\n",
    "\n",
    "Input: John Smith's neurology office (Happy Brain Services LLC) at 512 Example Street Suite 3600 (Camp Hill) is terrible!\n",
    "Output: ```json\n",
    "{\"first\": \"John\", \"last\": \"Smith\", \"specialty\": \"neurology\", \"legal_name\": \"Happy Brain Services LLC\", \"city_name\": \"Camp Hill\", \"zip\": null, \"addr\": \"512 Example Street\", \"addr2\": \"Suite 3600\"}\n",
    "```\n",
    "\n",
    "Input: Loved visiting Dr. Kay Ryan's neurology office (Green Medicine Inc.) at 256 Overflow St (ZIP 17011-2202)! No wait time at all. #CampHill\n",
    "Output: ```json\n",
    "{\"first\": \"Kay\", \"last\": \"Ryan\", \"specialty\": \"neurology\", \"legal_name\": \"Green Medicine Inc\", \"city_name\": \"Camp Hill\", \"zip\": \"170112202\", \"addr\": \"256 Overflow St\", \"addr2\": null}\n",
    "```\n",
    "\n",
    "Input: Dr. Pat Rogers's office screwed us! So much for Soulful Medical Services Inc. Took our money and Pat gave us three minutes tops. #BaltimoreSucks\n",
    "Output: ```json\n",
    "{\"first\": \"Pat\", \"last\": \"Rogers\", \"specialty\": null, \"legal_name\": \"Soulful Medical Services Inc\", \"city_name\": \"Baltimore\", \"zip\": null, \"addr\": null, \"addr2\": null}\n",
    "```\n",
    "\n",
    "Input: Dr. Maynard's really rules! They figured out my illness when no one else could. Loving the care here in #Lexington\n",
    "Output: ```json\n",
    "{\"first\": null, \"last\": \"Maynard\", \"specialty\": null, \"legal_name\": null, \"city_name\": \"Lexington\", \"zip\": null, \"addr\": null, \"addr2\": null}\n",
    "```\n",
    "\n",
    "The following is your input sentence. Produce the appropriate output.\n",
    "\n",
    "Input: $sentence\n",
    "Output:\"\"\"\n",
    ")\n",
    "# scrapped\n",
    "# The doctor's name is $name. Their office address is $address. Their specialty is $specialty. The city is $city.\n",
    "\n",
    "def format_json_prompt2(sentence: str) -> str:\n",
    "    return json_prompt2.substitute(sentence=sentence)\n",
    "\n",
    "print(json_prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f6acc-33f3-45e8-942c-56fca0573932",
   "metadata": {},
   "source": [
    "### JSON grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b424cb-7da8-405b-a773-6801ee903b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_grammar = r\"\"\"\n",
    "start: _form_no_fence\n",
    "_form_no_fence: _ASSISTANT_TAG NL NL? NL? json\n",
    "_ASSISTANT_TAG: \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "json: \"{\" NL? OPTIONAL_WS kv_pair (\",\" NL? OPTIONAL_WS kv_pair) ~ 0..7 NL? OPTIONAL_WS \"}\"\n",
    "kv_pair: COLUMN OPTIONAL_WS \":\" OPTIONAL_WS value\n",
    "value: STRING | NULL\n",
    "\n",
    "CODE_FENCE: \"```\"\n",
    "JSON_TAG: \"json\"\n",
    "OPTIONAL_WS: WS? WS? WS? WS?\n",
    "WS: \" \"\n",
    "NL: \"\\\\n\"\n",
    "NULL: \"null\"\n",
    "STRING: /\"[a-zA-Z0-9-.,'& ]{0,60}\"/\n",
    "COLUMN: \"\\\"\" (\"first\" | \"last\" | \"addr\" | \"addr2\" | \"specialty\" | \"city_name\" | \"zip\" | \"legal_name\") \"\\\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6bebff-3170-46b3-8f25-5d97e16033a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_grammar2 = \"\"\"\n",
    "start: _form_no_fence\n",
    "_form_no_fence: _ASSISTANT_TAG NL NL? NL? json\n",
    "_ASSISTANT_TAG: \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "json: \"{\" NL? OPTIONAL_WS kv_pair{FIRST, value{/\"[a-zA-Z -]{0,20}\"/}} NL? OPTIONAL_WS kv_pair{LAST, value{/[a-zA-Z -]{0,20}/}} NL? OPTIONAL_WS kv_pair{SPECIALTY, value{/\"[a-zA-Z]{0,20}\"/}} NL? OPTIONAL_WS kv_pair{LEGAL_NAME, value{/\"[a-zA-Z &.-]{0,50}\"/}} NL? OPTIONAL_WS kv_pair{CITY_NAME, value{/\"[a-zA-Z -]{0,30}/}} NL? OPTIONAL_WS kv_pair{ZIP, value{/|([0-9]{5,9})/}} NL? OPTIONAL_WS kv_pair{ADDR, value{/\"[a-zA-Z0-9 &.-]{0,50}\"/}} NL? OPTIONAL_WS kv_pair{ADDR2, value{/\"[a-zA-Z0-9 .-]{0,20}\"/}} NL? OPTIONAL_WS \"}\"\n",
    "kv_pair{col_, val}: col_ OPTIONAL_WS \":\" OPTIONAL_WS val\n",
    "value{something}: something | NULL\n",
    "\n",
    "CODE_FENCE: \"```\"\n",
    "JSON_TAG: \"json\"\n",
    "OPTIONAL_WS: WS? WS? WS? WS?\n",
    "WS: \" \"\n",
    "NL: \"\\\\n\"\n",
    "NULL: \"null\"\n",
    "STRING: /\"[a-zA-Z0-9-.,'& ]{0,60}\"/\n",
    "col{name}: \"\\\\\"\" name \"\\\\\"\"\n",
    "FIRST: col{\"first\"}\n",
    "LAST: col{\"last\"}\n",
    "SPECIALTY: col{\"specialty\"}\n",
    "LEGAL_NAME: col{\"legal_name\"}\n",
    "CITY_NAME: col{\"city_name\"}\n",
    "ZIP: col{\"zip\"}\n",
    "ADDR: col{\"addr\"}\n",
    "ADDR2: col{\"addr2\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12002d31-981c-4c93-aa4a-7bf7f49585e4",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348955fb-b8d0-4fb6-ab0e-0ec11e043fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Meta-Llama-3.1-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38336b-6a6e-43a6-bad4-5b187cc97de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_html_output_text_for_json(text: str) -> str:\n",
    "    return text.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace(\"|\", \"\\|\").replace(\"\\n\", \"<br>\").replace(\"```\", \"\\\\```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd551ff3-e963-47db-8902-53ae3024a844",
   "metadata": {},
   "source": [
    "### Running the LM unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27ed9a-93fc-44d1-94a8-2ab36faff129",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b0947-bfa3-4191-ba9c-e957ad3f2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from a magic number in the Genparse VLLM backend code.\n",
    "_MAX_MODEL_LEN = 4096\n",
    "# Why dtype=torch.float32? It's in the Genparse VLLM backend code.\n",
    "model = vllm.LLM(model_id, dtype=torch.float32, max_model_len=_MAX_MODEL_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b3407-04bd-4c33-88ee-1b3a87579c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952ea0d-8e46-42b2-a884-96caa19b1db4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = debug_sentences[-1][\"sentence\"]\n",
    "print(sentence)\n",
    "people = get_people(sentence)\n",
    "prompt = tokenizer.apply_chat_template([{'role': 'user', 'content': format_json_prompt(sentence=sentence)}], tokenize=False)\n",
    "print(f'Prompt ({len(tokenizer(prompt)[\"input_ids\"])} tokens): ```{prompt}```')\n",
    "\n",
    "sampling_params = vllm.SamplingParams(temperature=1.0, max_tokens=128, n=25)\n",
    "response = model.generate(prompt, sampling_params=sampling_params)[0]\n",
    "for i, output in enumerate(response.outputs, start=1):\n",
    "    print(f'Generated Query {i}: ```{output.text}```')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a81fa-3d9b-4c6c-a934-f815e9a9e5b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_to_outputs = {}\n",
    "for sentence_datum in debug_sentences:\n",
    "    sentence = sentence_datum[\"sentence\"]\n",
    "    print(sentence)\n",
    "    prompt = tokenizer.apply_chat_template([{'role': 'user', 'content': format_json_prompt2(sentence=sentence)}], tokenize=False)\n",
    "    logger.debug('Prompt (%d tokens): ```%s```', len(tokenizer(prompt)[\"input_ids\"]), prompt)\n",
    "    \n",
    "    sampling_params = vllm.SamplingParams(temperature=1.0, max_tokens=128, n=25)\n",
    "    response = model.generate(prompt, sampling_params=sampling_params)[0]\n",
    "    sentence_to_outputs[sentence] = []\n",
    "    for i, output in enumerate(response.outputs, start=1):\n",
    "        sentence_to_outputs[sentence].append(output)\n",
    "        print(f'Generated Query {i}: ```{output.text}```')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e025cdb-10f7-4207-b3d2-5d278b1b72f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table_rows = ['| Sentence | Extracted Code | Raw Response | # Tokens | Aggregate Likelihood (Raw LH) |', '|:--- |:--- |:--- | ---:| ---:|']\n",
    "for sentence, outputs in sentence_to_outputs.items():\n",
    "    try:\n",
    "        posterior = {}\n",
    "        n_outputs = {}\n",
    "        total_tokens = {}\n",
    "        for i, output in enumerate(outputs):\n",
    "            posterior.setdefault(output.text, 0.0)\n",
    "            posterior[output.text] += 1.0 / len(outputs)\n",
    "            n_outputs.setdefault(output.text, 0)\n",
    "            n_outputs[output.text] += 1\n",
    "            total_tokens.setdefault(output.text, 0)\n",
    "            total_tokens[output.text] += len(output.token_ids)\n",
    "        aggregate_likelihoods = sort_posterior(get_aggregate_likelihoods(posterior))\n",
    "        n_outputs_aggregated = {}\n",
    "        total_tokens_aggregated = {}\n",
    "        for output in outputs:\n",
    "            code = extract_code_from_response(output.text)\n",
    "            try:\n",
    "                code = json.dumps({key: value for key, value in sorted(json.loads(code).items())}, indent=2)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "            n_outputs_aggregated.setdefault(code, 0)\n",
    "            n_outputs_aggregated[code] += n_outputs[output.text]\n",
    "            total_tokens_aggregated.setdefault(code, 0)\n",
    "            total_tokens_aggregated[code] += total_tokens[output.text]\n",
    "        for i, (code, likelihood) in enumerate(aggregate_likelihoods.items(), start=1):\n",
    "            aggregate_likelihood = aggregate_likelihoods[code]\n",
    "            code_with_br = code.replace(\"\\n\", \"<br>\")\n",
    "            table_rows.append(\n",
    "                f'| {sentence if i == 1 else \"   \"} '\n",
    "                f'| <pre>{code_with_br}</pre> '\n",
    "                f'| {total_tokens_aggregated[code] / n_outputs_aggregated[code]}'\n",
    "                f'| (unk) '\n",
    "                f'| {100 * aggregate_likelihood:.2f}% ({100 * likelihood:.2f}%) |'\n",
    "            )\n",
    "    except json.JSONDecodeError as e:\n",
    "        table_rows.append(\n",
    "            f'| {sentence if i == 1 else \"   \"} | ERROR: {e} | 100% |'\n",
    "        )\n",
    "display(Markdown('\\n'.join(table_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf219e-c8b3-4189-92ec-cacc1e8940e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2acd6f-5541-4d60-9d41-2228e41c4c56",
   "metadata": {},
   "source": [
    "### Running the model with constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a95ec-1069-4370-b123-d7cd3574cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328b588-b4da-4ccf-b885-551f26d77fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = debug_sentences[-1][\"sentence\"]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'user', 'content': format_json_prompt(sentence=sentence)}\n",
    "    ],\n",
    "    tokenize=False,\n",
    ")\n",
    "print(f'Prompt ({len(tokenizer(prompt)[\"input_ids\"])} tokens): ```{prompt}```')\n",
    "\n",
    "\n",
    "# ignored top_p=0.95\n",
    "server_inference_params = {'max_tokens': 128, 'n_particles': 15, 'temperature': 1.0, 'grammar': json_grammar2}\n",
    "response = run_inference_server(prompt, **server_inference_params)\n",
    "try:\n",
    "    data = response.json()\n",
    "    for i, (query, likelihood) in enumerate(data['posterior'].items(), start=1):\n",
    "        \n",
    "        print(f'Generated Query {i} (likelihood {100 * likelihood:.2f}%): ```{query}```')\n",
    "except json.JSONDecodeError:\n",
    "    print(response.status_code)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79257b5a-8df6-4822-8350-df36973d7124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responses = {}\n",
    "for sentence_datum in debug_sentences:\n",
    "    sentence = sentence_datum[\"sentence\"]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'user', 'content': format_json_prompt(sentence=sentence)}\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    logger.debug('Prompt (%d tokens): ```%s```', len(tokenizer(prompt)[\"input_ids\"]), prompt)\n",
    "\n",
    "    # ignored top_p=0.95\n",
    "    server_inference_params = {'max_tokens': 128, 'n_particles': 15, 'temperature': 1.0, 'grammar': json_grammar}\n",
    "    response = run_inference_server(prompt, **server_inference_params)\n",
    "    responses[sentence] = response\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f00e1-7ea2-4883-8d48-ecf3fe93eb2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table_rows = ['| Sentence | Extracted Code | Raw Response | Aggregate Likelihood (Raw LH) |', '|:--- |:--- |:--- | ---:|']\n",
    "for sentence, response in responses.items():\n",
    "    try:\n",
    "        data = response.json()\n",
    "        posterior = data['posterior']\n",
    "        aggregate_likelihoods = get_aggregate_likelihoods(posterior)\n",
    "        # for i, (output_text, likelihood) in enumerate(posterior.items(), start=1):\n",
    "        #     # Assert the response parses properly\n",
    "        #     pclean_parser.parse(output_text)\n",
    "\n",
    "        #     code = extract_code_from_response(output_text)\n",
    "        #     aggregate_likelihood = aggregate_likelihoods[code]\n",
    "        #     code_with_br = code.replace(\"\\n\", \"<br>\")\n",
    "        #     output_with_br = md_html_output_text(output_text)\n",
    "        #     table_rows.append(\n",
    "        #         f'| {sentence if i == 1 else \"   \"} '\n",
    "        #         f'| <pre>{code_with_br}</pre> '\n",
    "        #         f'| <pre>{output_with_br}</pre> '\n",
    "        #         f'| {100 * aggregate_likelihood:.2f}% ({100 * likelihood:.2f}%) |'\n",
    "        #     )\n",
    "        for i, (code, likelihood) in enumerate(aggregate_likelihoods.items(), start=1):\n",
    "            aggregate_likelihood = aggregate_likelihoods[code]\n",
    "            code_with_br = code.replace(\"\\n\", \"<br>\")\n",
    "            table_rows.append(\n",
    "                f'| {sentence if i == 1 else \"   \"} '\n",
    "                f'| <pre>{code_with_br}</pre> '\n",
    "                f'| (unk) '\n",
    "                f'| {100 * aggregate_likelihood:.2f}% ({100 * likelihood:.2f}%) |'\n",
    "            )\n",
    "    except json.JSONDecodeError:\n",
    "        table_rows.append(\n",
    "            f'| {sentence if i == 1 else \"   \"} | ERROR: {response.status_code} - {response.text} | 100% |'\n",
    "        )\n",
    "display(Markdown('\\n'.join(table_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31bb02-1b9d-4d73-9b98-f48a21b51f31",
   "metadata": {},
   "source": [
    "### Local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ed3a9-51f0-4553-ae3d-756423ce96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_setup = server_model(json_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4891c0f-e05e-47b5-89fd-172555b1fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inference_setup.sampler.llm._model.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b16daf-83ed-42f5-9016-dbe78379b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(inference_setup.sampler.llm._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6bc605-9114-4e4f-8771-55b84d54001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = {}\n",
    "for sentence_datum in debug_sentences:\n",
    "    sentence = sentence_datum[\"sentence\"]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'user', 'content': format_json_prompt(sentence=sentence)}\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    logger.debug('Prompt (%d tokens): ```%s```', len(tokenizer(prompt)[\"input_ids\"]), prompt)\n",
    "\n",
    "    # ignored top_p=0.95\n",
    "    server_inference_params = {'max_tokens': 128, 'n_particles': 15, 'temperature': 1.0, 'grammar': json_grammar}\n",
    "    response = run_inference_vllm(prompt, inference_setup=inference_setup, **server_inference_params)\n",
    "    responses[sentence] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fcfd1b-0ed1-4a80-91f6-dbcfe8746554",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del inference_setup\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c681a-4bbb-4895-81f6-23f9e3c23b5c",
   "metadata": {},
   "source": [
    "### Running the model on an arbitrary sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495e27c3-7021-4101-b51b-b006597a2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = {}\n",
    "for sentence_datum in tqdm([#{\"sentence\": \"John Smith the podiatrist is terrible, so much worse than John Smith the cardiologist.\"}, {\"sentence\": \"John Smith works with Jack Smith at the neurology office. He’s terrible at neurology.\"},\n",
    "                            {\"sentence\": \"John Smith's office (Happy Brain Services LLC) at 512 Example Street Suite 3600 (CA-170) is terrible!\"}]):\n",
    "    sentence = sentence_datum[\"sentence\"]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'user', 'content': format_json_prompt(sentence=sentence)}\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    logger.debug('Prompt (%d tokens): ```%s```', len(tokenizer(prompt)[\"input_ids\"]), prompt)\n",
    "\n",
    "    # ignored top_p=0.95\n",
    "    server_inference_params = {'max_tokens': 256, 'n_particles': 15, 'temperature': 1.0, 'grammar': json_grammar}\n",
    "    response = run_inference_server(prompt, **server_inference_params)\n",
    "    responses[sentence] = response\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54661c9b-43d5-4566-809b-e5952f9705d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rows = ['| Sentence | Extracted JSON | Raw Response | Aggregate Likelihood (Raw LH) |', '|:--- |:--- |:--- | ---:|']\n",
    "for sentence, response in responses.items():\n",
    "    try:\n",
    "        data = response.json()\n",
    "        posterior = data['posterior']\n",
    "        for i, (output_text, likelihood) in enumerate(posterior.items(), start=1):\n",
    "            # Assert the response parses properly\n",
    "            # TODO grammar no longer parses in lark but does in genparse so idk, can't test this as easily\n",
    "            #json_parser.parse(output_text)\n",
    "\n",
    "            #code = extract_code_from_response(output_text)\n",
    "            code = output_text\n",
    "            code_with_br = code.replace(\"\\n\", \"<br>\")\n",
    "            output_with_br = md_html_output_text_for_json(output_text)\n",
    "            table_rows.append(\n",
    "                f'| {sentence if i == 1 else \"   \"} '\n",
    "                f'| <pre></pre> '\n",
    "                f'| <pre>{output_with_br}</pre> '\n",
    "                f'| {100 * likelihood:.2f}% ({100 * likelihood:.2f}%) |'\n",
    "            )\n",
    "    except json.JSONDecodeError:\n",
    "        table_rows.append(\n",
    "            f'| {sentence if i == 1 else \"   \"} | ERROR: {response.status_code} - {response.text} | 100% |'\n",
    "        )\n",
    "display(Markdown('\\n'.join(table_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e8bd2-8b01-445e-9872-d2cc081a8d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
