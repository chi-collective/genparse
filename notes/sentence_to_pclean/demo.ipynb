{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dce2b33-6852-4f53-9e91-22ecedb2f381",
   "metadata": {},
   "source": [
    "# Sentence to PClean query demo\n",
    "This notebook demos the sentence-to-PClean pipeline. Given a sentence giving information about some doctor, it generates and runs queries about those people against the Medicare dataset used in Alex Lew's paper using a PClean model trained by Ian Limarta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ae16ed-e3b5-441d-8f0a-5d78b9fb835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import string\n",
    "from typing import Any, Optional, TypeVar, Union\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import lark\n",
    "import nest_asyncio\n",
    "import requests\n",
    "import spacy\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import vllm\n",
    "\n",
    "from genparse import InferenceSetup, InferenceSetupVLLM\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "logger = logging.getLogger(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "088033b5-c175-43e6-9332-5ffa2aa9cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_sentences_path = Path().resolve() / \"debug_sentences.jsonl\"\n",
    "debug_sentences = [json.loads(line) for line in debug_sentences_path.read_text(encoding=\"utf-8\").splitlines() if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e71ec4e-1a6a-4c65-9192-eaf10cc559fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_debug_sentences_path = Path().resolve() / \"debug_sentences_detailed.jsonl\"\n",
    "detailed_debug_sentences = [json.loads(line) for line in detailed_debug_sentences_path.read_text(encoding=\"utf-8\").splitlines() if line.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd6668f-bba0-4fff-96dd-2dbd6355c851",
   "metadata": {},
   "source": [
    "## Example inputs and outputs\n",
    "\n",
    "Taken from the first ten sentences (rows 2-11) in the [GPT-4 tweets Marjorie Freedman generated][gpt4_tweets].\n",
    "\n",
    "[gpt4_tweets]: https://docs.google.com/spreadsheets/d/1vq_HAdbFY079vYVOppkFLk_zojJ46MJKsy-TzUm7Q3M/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c079f1-91a3-4c2b-a075-56c460f12d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Sentence | Example PClean preamble code |\n",
       "| --- | --- |\n",
       "| Just had an insightful consultation with Dr. Kay Ryan in Baltimore. Feeling optimistic about my health! #HealthMatters #Baltimore | <pre>name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)<br>row_trace[name_key] = \"Kay Ryan\"<br>city_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)<br>row_trace[city_key] = \"Baltimore\"</pre> |\n",
       "| Dr. Kay Ryan's office at 321 Pine St. in Baltimore is so welcoming and efficient. Highly recommend! #CityCare | <pre>name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)<br>row_trace[name_key] = \"Kay Ryan\"<br>city_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)<br>row_trace[city_key] = \"Baltimore\"<br>address_key = PClean.resolve_dot_expression(trace.model, :Obs, :address)<br>row_trace[address_key] = \"321 Pine St.\"</pre> |\n",
       "| Exploring Baltimore after an informative appointment with Dr. Kay Ryan. Loving this city's vibe! #BaltimoreAdventures #DoctorVisit | <pre>name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)<br>row_trace[name_key] = \"Kay Ryan\"<br>city_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)<br>row_trace[city_key] = \"Baltimore\"</pre> |\n",
       "| Dr. Kay Ryan, a neurologist, provided excellent care today. Grateful for her expertise! #Healthcare #BaltimoreDoctors | <pre>name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)<br>row_trace[name_key] = \"Kay Ryan\"<br>city_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)<br>row_trace[city_key] = \"Baltimore\"<br>occupation_key = PClean.resolve_dot_expression(trace.model, :Obs, :occupation)<br>row_trace[occupation_key] = \"neurologist\"</pre> |\n",
       "| Feeling reassured after my visit to Dr. Kay Ryan in Baltimore. She's truly exceptional! #FeelingGood #CityOfCharm | <pre>name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)<br>row_trace[name_key] = \"Kay Ryan\"<br>city_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)<br>row_trace[city_key] = \"Baltimore\"</pre> |\n",
       "| Had a great experience with Dr. Kay Ryan at 321 Pine St. in Baltimore. Her attention to detail is unmatched! #HealthCheck | <pre>name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)<br>row_trace[name_key] = \"Kay Ryan\"<br>city_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)<br>row_trace[city_key] = \"Baltimore\"<br>address_key = PClean.resolve_dot_expression(trace.model, :Obs, :address)<br>row_trace[address_key] = \"321 Pine St.\"</pre> |\n",
       "| From diagnosis to treatment, Dr. Kay Ryan, a cardiologist, covered it all. Baltimore, you're lucky to have her! #MedicalCare | <pre>name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)<br>row_trace[name_key] = \"Kay Ryan\"<br>city_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)<br>row_trace[city_key] = \"Baltimore\"<br>occupation_key = PClean.resolve_dot_expression(trace.model, :Obs, :occupation)<br>row_trace[occupation_key] = \"cardiologist\"</pre> |\n",
       "| Just wrapped up my appointment with Dr. Kay Ryan in Baltimore. Her professionalism is top-notch. #HealthJourney | <pre>name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)<br>row_trace[name_key] = \"Kay Ryan\"<br>city_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)<br>row_trace[city_key] = \"Baltimore\"</pre> |\n",
       "| Dr. Kay Ryan's office in Baltimore is so efficient and welcoming. A great experience overall! #DoctorVisit #CityCare | <pre>name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)<br>row_trace[name_key] = \"Kay Ryan\"<br>city_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)<br>row_trace[city_key] = \"Baltimore\"</pre> |\n",
       "| Can't say enough good things about Dr. Kay Ryan, a pediatrician, in Baltimore. Truly an outstanding doctor! #Grateful | <pre>name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)<br>row_trace[name_key] = \"Kay Ryan\"<br>city_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)<br>row_trace[city_key] = \"Baltimore\"<br>occupation_key = PClean.resolve_dot_expression(trace.model, :Obs, :occupation)<br>row_trace[occupation_key] = \"pediatrician\"</pre> |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_table_row_template = string.Template(\"| $sentence | <pre>$example_output</pre> |\")\n",
    "def show_sentences(sentence_data):\n",
    "    lines = [\"| Sentence | Example PClean preamble code |\", \"| --- | --- |\"]\n",
    "    for sentence_datum in sentence_data:\n",
    "        lines.append(sentence_table_row_template.substitute(sentence=sentence_datum[\"sentence\"], example_output=sentence_datum[\"example_output\"].replace(\"\\n\", \"<br>\")))\n",
    "    markdown = \"\\n\".join(lines)\n",
    "    display(Markdown(markdown))\n",
    "\n",
    "\n",
    "show_sentences(debug_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03784340-c67a-485c-9624-77ef5936d5e7",
   "metadata": {},
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bffc6b-ec42-4b0c-8a92-9f68830032eb",
   "metadata": {},
   "source": [
    "### SpaCy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad06c20-4e09-4ca2-84af-285dd23c3dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just had an insightful consultation with Dr. Kay Ryan in Baltimore. Feeling optimistic about my health! #HealthMatters #Baltimore\n",
      "(Kay Ryan, Baltimore, Baltimore)\n",
      "\n",
      "Dr. Kay Ryan's office at 321 Pine St. in Baltimore is so welcoming and efficient. Highly recommend! #CityCare\n",
      "(Kay Ryan, Baltimore, CityCare)\n",
      "\n",
      "Exploring Baltimore after an informative appointment with Dr. Kay Ryan. Loving this city's vibe! #BaltimoreAdventures #DoctorVisit\n",
      "(Baltimore, Kay Ryan)\n",
      "\n",
      "Dr. Kay Ryan, a neurologist, provided excellent care today. Grateful for her expertise! #Healthcare #BaltimoreDoctors\n",
      "(Kay Ryan, today)\n",
      "\n",
      "Feeling reassured after my visit to Dr. Kay Ryan in Baltimore. She's truly exceptional! #FeelingGood #CityOfCharm\n",
      "(Kay Ryan, Baltimore)\n",
      "\n",
      "Had a great experience with Dr. Kay Ryan at 321 Pine St. in Baltimore. Her attention to detail is unmatched! #HealthCheck\n",
      "(Kay Ryan, Baltimore)\n",
      "\n",
      "From diagnosis to treatment, Dr. Kay Ryan, a cardiologist, covered it all. Baltimore, you're lucky to have her! #MedicalCare\n",
      "(Kay Ryan, Baltimore)\n",
      "\n",
      "Just wrapped up my appointment with Dr. Kay Ryan in Baltimore. Her professionalism is top-notch. #HealthJourney\n",
      "(Kay Ryan, Baltimore)\n",
      "\n",
      "Dr. Kay Ryan's office in Baltimore is so efficient and welcoming. A great experience overall! #DoctorVisit #CityCare\n",
      "(Kay Ryan, Baltimore)\n",
      "\n",
      "Can't say enough good things about Dr. Kay Ryan, a pediatrician, in Baltimore. Truly an outstanding doctor! #Grateful\n",
      "(Kay Ryan, Baltimore)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Supporting code for entity extraction.\n",
    "_SPACY_MODEL_NAME = 'en_core_web_trf'\n",
    "spacy_model = spacy.load(_SPACY_MODEL_NAME)\n",
    "_PERSON_LABEL = 'PERSON'\n",
    "_LOCATION_LABEL = 'LOC'\n",
    "# spaCy labels list example thanks to Stack Overflow user 'russhoppa': https://stackoverflow.com/a/78252807\n",
    "_SPACY_LABELS = spacy_model.get_pipe(\"ner\").labels\n",
    "assert _PERSON_LABEL in _SPACY_LABELS\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "def _uniquify(items: list[T]) -> list[T]:\n",
    "    \"\"\"\n",
    "    O(n^2) order-preserving uniquification.\n",
    "\n",
    "    Fine for short lists like a single sentence's list of PERSON entities.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for item in items:\n",
    "        if item not in result:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "def get_people(sentence: str) -> list[str]:\n",
    "    return _uniquify(\n",
    "        [ent.text for ent in spacy_model(sentence).ents if ent.label_ == _PERSON_LABEL]\n",
    "    )\n",
    "\n",
    "def get_locations(sentence: str) -> list[str]:\n",
    "    return _uniquify(\n",
    "        [ent.text for ent in spacy_model(sentence).ents if ent.label_ == _LOCATION_LABEL]\n",
    "    )\n",
    "\n",
    "def show_ents(sentence: str) -> None:\n",
    "    print(sentence)\n",
    "    print(spacy_model(sentence).ents)\n",
    "\n",
    "for sentence_datum in debug_sentences:\n",
    "    show_ents(sentence_datum[\"sentence\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6069e-d97c-4b8f-8697-62a9148bdfac",
   "metadata": {},
   "source": [
    "### Extracting the generated code from a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5444b2ba-605d-45cf-b7c5-57073c132990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_from_response(text: str) -> str:\n",
    "    try:\n",
    "        fence = '```'\n",
    "        start = text.index(fence) + len(fence)\n",
    "        julia_bit = 'julia\\n'\n",
    "        json_bit = 'json\\n'\n",
    "        if text[start:].startswith(julia_bit):\n",
    "            start += len(julia_bit)\n",
    "        elif text[start:].startswith(json_bit):\n",
    "            start += len(json_bit)\n",
    "        end = text.rindex('```')\n",
    "        result = text[start:end].strip()\n",
    "    except IndexError:\n",
    "        result = text.removeprefix('<|start_header_id|>assistant<|end_header_id|>').strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eacdc399-550e-4e88-9f98-056d85b7657d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)\\nrow_trace[name_key] = \"Kay Ryan\"\\ncity_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)\\nrow_trace[city_key] = \"Baltimore\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test to confirm it works as intended\n",
    "text = '''<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "Here is the Julia code to query the PClean table of records about doctors based on the given input sentence:\n",
    "\n",
    "```julia\n",
    "name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)\n",
    "row_trace[name_key] = \"Kay Ryan\"\n",
    "city_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)\n",
    "row_trace[city_key] = \"Baltimore\"\n",
    "```'''\n",
    "extract_code_from_response(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b9dc8-ce6b-413b-a32a-6f58f05c3a15",
   "metadata": {},
   "source": [
    "### Sort posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b87cf18-b455-4de3-8aae-75d3ec3a95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_posterior(posterior):\n",
    "    return {inference: likelihood for inference, likelihood in sorted(posterior.items(), key=lambda t: (t[1], t[0]), reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c8f11-660c-4c9e-9918-3a36cbf3535a",
   "metadata": {},
   "source": [
    "### Aggregate likelihoods over extracted code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c65a7792-3619-4df0-9128-ed85c41dcc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_likelihoods(posterior: dict[str, float]) -> dict[str, float]:\n",
    "    result = {}\n",
    "    for inference, likelihood in posterior.items():\n",
    "        code_only = extract_code_from_response(inference)\n",
    "        result.setdefault(code_only, 0.0)\n",
    "        result[code_only] += likelihood\n",
    "    return sort_posterior(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b68b56-db12-4264-a14b-de533be8e5c5",
   "metadata": {},
   "source": [
    "### Get best inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a463c9d-5e48-4bdb-b6d8-0e9f8ea069ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_inference(posterior: dict[str, float]) -> tuple[str, float]:\n",
    "    return max(posterior.items(), key=lambda t: (t[1], t[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67b831-8d2e-401d-9523-6f9602c74151",
   "metadata": {},
   "source": [
    "### Running inference locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a725710-f405-4b3a-a101-93b3a3256e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER_MODEL_ID = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "SERVER_GENPARSE_MODEL_NAME = \"llama3\"\n",
    "_LOCAL_BATCH_SIZE = 1\n",
    "\n",
    "class FakeResponse:\n",
    "    def __init__(self, data):\n",
    "        self._data = data\n",
    "    \n",
    "    @property\n",
    "    def status_code(self):\n",
    "        return 200\n",
    "\n",
    "    def json(self):\n",
    "        return self._data\n",
    "\n",
    "    def text(self):\n",
    "        return json.dumps(self._data)\n",
    "\n",
    "def server_model(grammar, proposal=\"character\"):\n",
    "    return InferenceSetupVLLM(SERVER_GENPARSE_MODEL_NAME, grammar, proposal_name=proposal)\n",
    "\n",
    "def run_inference_vllm(\n",
    "    prompt: str,\n",
    "    *,\n",
    "    proposal: str = 'character',\n",
    "    batch_size: int = _LOCAL_BATCH_SIZE,\n",
    "    max_tokens: int,\n",
    "    n_particles: int,\n",
    "    temperature: float = 1.0,\n",
    "    grammar: str,\n",
    "    genparse_url: str = \"\",\n",
    "    inference_setup: InferenceSetupVLLM,\n",
    ") -> requests.Response:\n",
    "    assert batch_size == inference_setup.batch_size\n",
    "    assert grammar == inference_setup.grammar\n",
    "    assert proposal == inference_setup.proposal_name\n",
    "    inference = inference_setup(prompt, n_particles=n_particles, temperature=temperature, max_tokens=max_tokens)\n",
    "    return FakeResponse({\"log_ml\": None, \"posterior\": inference.posterior})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f54459-5000-44c3-90a7-01c817e87b29",
   "metadata": {},
   "source": [
    "### Running the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "423fe5b1-ad77-46bb-8d3e-1bc026e0d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_BATCH_SIZE = 1\n",
    "# Ben LeBrun's WIP server running on GCP as of 2024-07-16\n",
    "_DEFAULT_GENPARSE_INFERENCE_SERVER_URI = 'http://34.122.30.137:8888/infer'\n",
    "\n",
    "def run_inference_server(\n",
    "    prompt: str,\n",
    "    *,\n",
    "    proposal: str = 'character',\n",
    "    batch_size: int = _BATCH_SIZE,\n",
    "    max_tokens: int,\n",
    "    n_particles: int,\n",
    "    temperature: float = 1.0,\n",
    "    grammar: str,\n",
    "    genparse_url: str = _DEFAULT_GENPARSE_INFERENCE_SERVER_URI,\n",
    ") -> requests.Response:\n",
    "    \"\"\"\n",
    "    Run inference using a server.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'prompt': prompt,\n",
    "        'method': 'smc-standard',\n",
    "        'n_particles': n_particles,\n",
    "        'lark_grammar': grammar,\n",
    "        'proposal_name': proposal,\n",
    "        'proposal_args': {},\n",
    "        'max_tokens': max_tokens,\n",
    "        'temperature': temperature,\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    response = requests.post(genparse_url, json=params, headers=headers)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b9f1d-e42f-4ccb-a5c2-3a34bcd219a6",
   "metadata": {},
   "source": [
    "### PClean grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a849763-6897-4859-aab4-53758a0965de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pclean_grammar = r\"\"\"\n",
    "start: prefix julia_code suffix\n",
    "prefix: \"<|start_header_id|>assistant<|end_header_id|>\" NL* FREE_TEXT? NL+ CODE_FENCE JULIA? NL\n",
    "suffix: NL CODE_FENCE\n",
    "julia_code: add_to_trace (NL+ add_to_trace)+\n",
    "\n",
    "FREE_TEXT: /[a-zA-Z0-9.,-?!;: ]+/\n",
    "CODE_FENCE: \"```\"\n",
    "JULIA: \"julia\"\n",
    "WS: \" \"\n",
    "NL: \"\\n\"\n",
    "STRING: /\"[a-zA-Z0-9. ]*\"/\n",
    "\n",
    "add_to_trace: get_key NL set_key_in_trace\n",
    "# slightly overly restrictive but good enough \n",
    "get_key: trace_key_identifier WS* \"=\" WS* \"PClean.resolve_dot_expression(trace.model, :Obs, \" column_symbol \")\"\n",
    "# column_symbol: /:[a-z][a-z_]+/\n",
    "column_symbol: \":\" (\"name\" | \"address\" | \"specialty\" | \"city\")\n",
    "set_key_in_trace: \"row_trace[\" trace_key_identifier \"]\" WS* \"=\" WS* STRING\n",
    "trace_key_identifier: /[a-z][a-z_]+/\n",
    "\"\"\"\n",
    "pclean_parser = lark.Lark(pclean_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e103fc-ee40-4b8d-8e39-934aacc85df6",
   "metadata": {},
   "source": [
    "### PClean code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f71de7e-c9dc-404d-aa64-6e676defb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pclean_template = string.Template(\n",
    "    \"\"\"\n",
    "# Create a new row trace for the hypothetical row\n",
    "row_trace = Dict{PClean.VertexID, Any}()\n",
    "$preamble\n",
    "\n",
    "# Add it to the trace\n",
    "obs = trace.tables[:Obs].observations\n",
    "row_id = gensym()\n",
    "obs[row_id] = row_trace\n",
    "\n",
    "samples = []\n",
    "for _ in 1:$N\n",
    "    # Perform a Partilce Gibbs MCMC move to change our current sample of the row\n",
    "    PClean.run_smc(!(trace, :Obs, row_id, PClean.InferenceConfig(1, 10))\n",
    "    # Accumulate the sample\n",
    "    push!(samples, trace.tables[:Obs].rows[row_id][br_idx]\n",
    "end\n",
    "\n",
    "countmap(samples)\n",
    "\"\"\".lstrip()\n",
    ")\n",
    "\n",
    "PCLEAN_DEFAULT_N_SAMPLES = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187f3d6-acc4-40c6-9fa9-931a4228ca03",
   "metadata": {},
   "source": [
    "### PClean generation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c407b4c-b8f6-4f45-bd33-b9d5488a5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pclean_prompt = string.Template(\n",
    "    \"\"\"Write Julia code to query a PClean table of records about doctors based on the given input sentence.\n",
    "\n",
    "In general, your output should look like pairs of lines:\n",
    "\n",
    "```julia\n",
    "blah_key = PClean.resolve_dot_expression(trace.model, :Obs, :blah_col)\n",
    "row_trace[blah_key] = \"Value of Blah as expressed in the sentence\"\n",
    "```\n",
    "\n",
    "The dataset has just four columns to query:\n",
    "\n",
    "- :name (the doctor's full name, first and last)\n",
    "- :city (the city where the doctor practices)\n",
    "- :address (the doctor's office address)\n",
    "- :specialty (the doctor's specialty)\n",
    "\n",
    "Please generate code to query all values specified in the sentence. Output the Julia code directly with no preamble or commentary. Write just two lines per column.\n",
    "\n",
    "Input: Loved visiting Dr. Kay Ryan's neurology office at 256 Overflow St! No wait time at all. #Baltimore\n",
    "Output: ```julia\n",
    "name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)\n",
    "row_trace[name_key] = \"Kay Ryan\"\n",
    "address_key = PClean.resolve_dot_expression(trace.model, :Obs, :address)\n",
    "row_trace[address_key] = \"256 Overflow St\"\n",
    "specialty_key = PClean.resolve_dot_expression(trace.model, :Obs, :specialty)\n",
    "row_trace[specialty_key] = \"neurology\"\n",
    "city_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)\n",
    "row_trace[city_key] = \"Baltimore\"\n",
    "```\n",
    "\n",
    "Input: Dr. Pat Rogers's orthopedics office screwed us! Took our money and Kay gave us three minutes tops. #BaltimoreSucks\n",
    "Output: ```julia\n",
    "name_key = PClean.resolve_dot_expression(trace.model, :Obs, :name)\n",
    "row_trace[name_key] = \"Pat Rogers\"\n",
    "specialty_key = PClean.resolve_dot_expression(trace.model, :Obs, :specialty)\n",
    "row_trace[specialty_key] = \"orthopedics\"\n",
    "city_key = PClean.resolve_dot_expression(trace.model, :Obs, :city)\n",
    "row_trace[city_key] = \"Baltimore\"\n",
    "```\n",
    "\n",
    "In other words, we query all values given in the sentence: The doctor's name, their office address, their specialty, and the city. If one of these is missing, we do not query on it.\n",
    "\n",
    "Input: $sentence\n",
    "Output:\"\"\"\n",
    ")\n",
    "# scrapped\n",
    "# The doctor's name is $name. Their office address is $address. Their specialty is $specialty. The city is $city.\n",
    "\n",
    "def format_people_prompt(sentence: str) -> str:\n",
    "    return pclean_prompt.substitute(sentence=sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c459c02c-cd06-4768-b133-9507c82cd21a",
   "metadata": {},
   "source": [
    "### Running PClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da54f347-de6b-442d-abb4-a675da04d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pclean(code: str) -> Any:\n",
    "    # TODO\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197ffdf-6321-4d8c-acf7-aeffa615ca53",
   "metadata": {},
   "source": [
    "### Displaying PClean query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f703684b-5449-4f1c-8b38-dacaf02bcc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_query_results(query_results: Any) -> None:\n",
    "    # TODO\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec71b2d-9800-4ab0-b9cd-922fa623e47c",
   "metadata": {},
   "source": [
    "### JSON generation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4555ae6e-7c54-410a-9ee2-de168caf6037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<string.Template object at 0x7f4b83ea1c30>\n"
     ]
    }
   ],
   "source": [
    "json_prompt = string.Template(\n",
    "    \"\"\"Write a flat JSON object describing one of the doctors in the given input sentence.\n",
    "\n",
    "In general, your output should look like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"some_feature\": \"foo\",\n",
    "    \"another_feature\": \"bar\",\n",
    "    ...\n",
    "    \"last_feature\": \"baz\"\n",
    "}\n",
    "```\n",
    "\n",
    "There are eight features we would like to extract from the sentence:\n",
    "\n",
    "- \"first\" (the doctor's first name)\n",
    "- \"last\" (the doctor's last name)\n",
    "- \"specialty\" (the doctor's specialty)\n",
    "- \"legal_name\" (the legal name of the doctor's business, practice, or employer)\n",
    "- \"city_name\" (the city where the doctor's office is)\n",
    "- \"zip\" (the ZIP code of the doctor's office)\n",
    "- \"addr\" (the doctor's office address, \"line 1\" -- this is the street address)\n",
    "- \"addr2\" (additional address information giving a suite number, unit number, et cetera -- \"line 2\" of the address)\n",
    "\n",
    "Please generate a key-value pair for every such value specified in the sentence. Avoid nulls and empty strings. Omit any missing values. Do not write a script. Output the JSON itself directly with no preamble or commentary. The following examples illustrate how you should behave on the input sentence.\n",
    "\n",
    "Input: John Smith's neurology office (Happy Brain Services LLC) at 512 Example Street Suite 3600 (Camp Hill) is terrible!\n",
    "Output: ```json\n",
    "{\"first\": \"John\", \"last\": \"Smith\", \"specialty\": \"neurology\", \"legal_name\": \"Happy Brain Services LLC\", \"addr\": \"512 Example Street\", \"addr2\": \"Suite 3600\", \"city_name\": \"Camp Hill\"}\n",
    "```\n",
    "\n",
    "Input: Loved visiting Dr. Kay Ryan's neurology office (Green Medicine Inc.) at 256 Overflow St (ZIP 17011-2202)! No wait time at all. #CampHill\n",
    "Output: ```json\n",
    "{\"first\": \"Kay\", \"last\": \"Ryan\", \"legal_name\": \"Green Medicine Inc\", \"specialty\": \"neurology\", \"addr\": \"256 Overflow St\", \"zip\": \"170112202\", \"city_name\": \"Camp Hill\"}\n",
    "```\n",
    "\n",
    "Input: Dr. Pat Rogers's office screwed us! So much for Soulful Medical Services Inc. Took our money and Pat gave us three minutes tops. #BaltimoreSucks\n",
    "Output: ```json\n",
    "{\"first\": \"Pat\", \"last\": \"Rogers\", \"legal_name\": \"Soulful Medical Services Inc\", \"city_name\": \"Baltimore\"}\n",
    "```\n",
    "\n",
    "The following is your input sentence. Produce the appropriate output.\n",
    "\n",
    "Input: $sentence\n",
    "Output:\"\"\"\n",
    ")\n",
    "# scrapped\n",
    "# The doctor's name is $name. Their office address is $address. Their specialty is $specialty. The city is $city.\n",
    "\n",
    "def format_json_prompt(sentence: str) -> str:\n",
    "    return json_prompt.substitute(sentence=sentence)\n",
    "\n",
    "print(json_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f6acc-33f3-45e8-942c-56fca0573932",
   "metadata": {},
   "source": [
    "### JSON grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d49c49b-ff7b-4e6d-8fa9-f9be0e89692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_grammar = r\"\"\"\n",
    "start: prefix json suffix\n",
    "prefix: \"<|start_header_id|>assistant<|end_header_id|>\" NL* FREE_TEXT? NL+ CODE_FENCE JSON_TAG? NL\n",
    "suffix: NL CODE_FENCE\n",
    "json: \"{\" NL? WS* kv_pair (\",\" NL? WS* kv_pair)* NL? WS* \"}\"\n",
    "kv_pair: COLUMN WS* \":\" WS* STRING\n",
    "\n",
    "FREE_TEXT: /[a-zA-Z0-9.,-?!;:' ]+/\n",
    "CODE_FENCE: \"```\"\n",
    "JSON_TAG: \"json\"\n",
    "WS: \" \"\n",
    "NL: \"\\n\"\n",
    "STRING: /\"[a-zA-Z0-9-.,'& ]*\"/\n",
    "# dropped for now: \"city\"\n",
    "COLUMN: \"\\\"\" (\"first_name\" | \"last_name\" | \"address\" | \"address2\" | \"specialty\" | \"c2z3\" | \"legal_name\") \"\\\"\"\n",
    "\"\"\"\n",
    "json_parser = lark.Lark(json_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a7e3e0d-1081-4641-82dc-12de4c2124aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_nonempty_json_grammar = r\"\"\"\n",
    "start: prefix json suffix\n",
    "prefix: \"<|start_header_id|>assistant<|end_header_id|>\" NL* FREE_TEXT? NL+ CODE_FENCE JSON_TAG? NL\n",
    "suffix: NL CODE_FENCE\n",
    "json: \"{\" NL? WS* kv_pair (\",\" NL? WS* kv_pair)* NL? WS* \"}\"\n",
    "#kv_pair: COLUMN WS* \":\" WS* STRING\n",
    "kv_pair: (NONEMPTY_COLUMN WS* \":\" WS* NONEMPTY_STRING) | (EMPTYABLE_COLUMN WS* \":\" WS* STRING)\n",
    "\n",
    "FREE_TEXT: /[a-zA-Z0-9.,-?!;:' ]+/\n",
    "CODE_FENCE: \"```\"\n",
    "JSON_TAG: \"json\"\n",
    "WS: \" \"\n",
    "NL: \"\\n\"\n",
    "NONEMPTY_STRING: /\"[a-zA-Z0-9-.,'& ]+\"/\n",
    "STRING: /\"[a-zA-Z0-9-.,'& ]*\"/\n",
    "NONEMPTY_COLUMN: \"\\\"\" (\"first\" | \"last\" | \"specialty | \"addr\" | \"city\" | \"zip\" | \"legal_name\") \"\\\"\"\n",
    "EMPTYABLE_COLUMN: \"\\\"addr2\\\"\"\n",
    "#COLUMN: \"\\\"\" (\"first\" | \"last\" | \"addr\" | \"addr2\" | \"specialty\" | \"city\" | \"zip\" | \"legal_name\") \"\\\"\"\n",
    "\"\"\"\n",
    "json_parser = lark.Lark(json_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74b424cb-7da8-405b-a773-6801ee903b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_grammar = r\"\"\"\n",
    "start: _form1 | _form2\n",
    "_form2: _ASSISTANT_TAG NL+ json\n",
    "_form1: prefix json suffix\n",
    "_ASSISTANT_TAG: \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "prefix: _ASSISTANT_TAG NL* FREE_TEXT? NL+ CODE_FENCE JSON_TAG? NL\n",
    "suffix: NL CODE_FENCE\n",
    "json: \"{\" NL? WS* kv_pair (\",\" NL? WS* kv_pair)* NL? WS* \"}\"\n",
    "kv_pair: COLUMN WS* \":\" WS* STRING\n",
    "\n",
    "FREE_TEXT: /[a-zA-Z0-9.,-?!;:' ]+/\n",
    "CODE_FENCE: \"```\"\n",
    "JSON_TAG: \"json\"\n",
    "WS: \" \"\n",
    "NL: \"\\n\"\n",
    "STRING: /\"[a-zA-Z0-9-.,'& ]*\"/\n",
    "# dropped for now: \"city\"\n",
    "COLUMN: \"\\\"\" (\"first\" | \"last\" | \"addr\" | \"addr2\" | \"specialty\" | \"city\" | \"zip\" | \"legal_name\") \"\\\"\"\n",
    "\n",
    "\"\"\"\n",
    "json_parser = lark.Lark(json_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec616e06-863d-41fe-a009-657589be0064",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b9cd6f-e926-4e5f-97a9-1d2d4da7bbdd",
   "metadata": {},
   "source": [
    "Example sentences:\n",
    "\n",
    "1. Just had an awful consultation with Dr. Kay Ryan in Baltimore. My heart has never felt worse -- Kay knows nothing! Get another cardiologist! #HealthMatters #Baltimore\n",
    "2. Dr. Kay Ryan's office at 384 Oak St. in Baltimore is so welcoming and efficient. Highly recommend! #CityCare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143cb94-36e9-4ee7-a05e-c9a0cc2f2d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 256\n",
    "N_PARTICLES = 15\n",
    "N_SAMPLES = 100\n",
    "server_inference_params = {'max_tokens': MAX_NEW_TOKENS, 'n_particles': N_PARTICLES, 'grammar': pclean_grammar}\n",
    "\n",
    "while True:\n",
    "    sentence = input('Give me a sentence involving a (named) physician (ideally with specialty, address, or city name): ')\n",
    "    if sentence == 'q':\n",
    "        break\n",
    "\n",
    "    print(f'Processing sentence `{sentence}`...')\n",
    "    people = get_people(sentence)\n",
    "    logger.debug(f'People: {people}')\n",
    "    \n",
    "    prompt = format_people_prompt(sentence=sentence)\n",
    "    logger.debug(f'Prompt: {prompt}')\n",
    "\n",
    "    try:\n",
    "        response = run_inference_server(prompt, **server_inference_params)\n",
    "        data = response.json()\n",
    "    except json.DecodeError as e:\n",
    "        print(f'ERROR: response status {response.status_code} - {response.text}. Parse error {e}.')\n",
    "        continue\n",
    "    aggregate_likelihoods = get_aggregate_likelihoods(data['posterior'])\n",
    "    best_code, likelihood = get_best_inference(aggregate_likelihoods)\n",
    "    full_code = pclean_template.substitute(preamble=best_code, N=N_SAMPLES)\n",
    "    \n",
    "    display(Markdown('Code:'))\n",
    "    display(Markdown(f'```julia\\n{full_code}\\n```'))\n",
    "    display(Markdown(f'Likelihood: {100 * likelihood:.2f}%'))\n",
    "    \n",
    "    try:\n",
    "        results = run_pclean(full_code)\n",
    "        display(Markdown('Query Results:'))\n",
    "        display_query_results(results)\n",
    "    except NotImplementedError:\n",
    "        print(\"Sorry, we can't actually run queries yet. 😅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12002d31-981c-4c93-aa4a-7bf7f49585e4",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd551ff3-e963-47db-8902-53ae3024a844",
   "metadata": {},
   "source": [
    "### Running the LM unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "348955fb-b8d0-4fb6-ab0e-0ec11e043fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a27ed9a-93fc-44d1-94a8-2ab36faff129",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b0947-bfa3-4191-ba9c-e957ad3f2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from a magic number in the Genparse VLLM backend code.\n",
    "_MAX_MODEL_LEN = 4096\n",
    "# Why dtype=torch.float32? It's in the Genparse VLLM backend code.\n",
    "model = vllm.LLM(model_id, dtype=torch.float32, max_model_len=_MAX_MODEL_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b48b3407-04bd-4c33-88ee-1b3a87579c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952ea0d-8e46-42b2-a884-96caa19b1db4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = debug_sentences[1][\"sentence\"]\n",
    "print(sentence)\n",
    "people = get_people(sentence)\n",
    "prompt = tokenizer.apply_chat_template([{'role': 'user', 'content': format_json_prompt(sentence=sentence)}], tokenize=False)\n",
    "print(f'Prompt ({len(tokenizer(prompt)[\"input_ids\"])} tokens): ```{prompt}```')\n",
    "\n",
    "sampling_params = vllm.SamplingParams(temperature=1.0, max_tokens=1024, n=25)\n",
    "response = model.generate(prompt, sampling_params=sampling_params)[0]\n",
    "for i, output in enumerate(response.outputs, start=1):\n",
    "    print(f'Generated Query {i}: ```{output.text}```')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e07057-4e99-494a-a3a0-cfefe4eefdd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sentence_datum in detailed_debug_sentences:\n",
    "    sentence = sentence_datum[\"sentence\"]\n",
    "    print(sentence)\n",
    "    prompt = tokenizer.apply_chat_template([{'role': 'user', 'content': format_json_prompt(sentence=sentence)}], tokenize=False)\n",
    "    print(f'Prompt ({len(tokenizer(prompt)[\"input_ids\"])} tokens): ```{prompt}```')\n",
    "    \n",
    "    sampling_params = vllm.SamplingParams(temperature=1.0, max_tokens=1024, n=25)\n",
    "    response = model.generate(prompt, sampling_params=sampling_params)[0]\n",
    "    for i, output in enumerate(response.outputs, start=1):\n",
    "        print(f'Generated Query {i}: ```{output.text}```')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf219e-c8b3-4189-92ec-cacc1e8940e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2acd6f-5541-4d60-9d41-2228e41c4c56",
   "metadata": {},
   "source": [
    "### Running the model with constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328b588-b4da-4ccf-b885-551f26d77fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = debug_sentences[1][\"sentence\"]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'user', 'content': format_json_prompt(sentence=sentence)}\n",
    "    ],\n",
    "    tokenize=False,\n",
    ")\n",
    "print(f'Prompt ({len(tokenizer(prompt)[\"input_ids\"])} tokens): ```{prompt}```')\n",
    "\n",
    "\n",
    "# ignored top_p=0.95\n",
    "server_inference_params = {'max_tokens': 128, 'n_particles': 15, 'temperature': 1.0, 'grammar': json_grammar}\n",
    "response = run_inference_server(prompt, **server_inference_params)\n",
    "try:\n",
    "    data = response.json()\n",
    "    for i, (query, likelihood) in enumerate(data['posterior'].items(), start=1):\n",
    "        \n",
    "        print(f'Generated Query {i} (likelihood {100 * likelihood:.2f}%): ```{query}```')\n",
    "except json.JSONDecodeError:\n",
    "    print(response.status_code)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7da693-af93-494c-8343-577fe9f15a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = debug_sentences[1][\"sentence\"]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'user', 'content': format_json_prompt(sentence=sentence)}\n",
    "    ],\n",
    "    tokenize=False,\n",
    ")\n",
    "print(f'Prompt ({len(tokenizer(prompt)[\"input_ids\"])} tokens): ```{prompt}```')\n",
    "\n",
    "\n",
    "# ignored top_p=0.95\n",
    "server_inference_params = {'max_tokens': 128, 'n_particles': 15, 'temperature': 1.0, 'grammar': json_grammar}\n",
    "response = run_inference_server(prompt, **server_inference_params)\n",
    "try:\n",
    "    data = response.json()\n",
    "    for i, (query, likelihood) in enumerate(data['posterior'].items(), start=1):\n",
    "        print(f'Generated Query {i} (likelihood {100 * likelihood:.2f}%): ```{query}```')\n",
    "except json.JSONDecodeError:\n",
    "    print(response.status_code)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79257b5a-8df6-4822-8350-df36973d7124",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = {}\n",
    "for sentence_datum in debug_sentences:\n",
    "    sentence = sentence_datum[\"sentence\"]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'user', 'content': format_json_prompt(sentence=sentence)}\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    logger.debug('Prompt (%d tokens): ```%s```', len(tokenizer(prompt)[\"input_ids\"]), prompt)\n",
    "\n",
    "    # ignored top_p=0.95\n",
    "    server_inference_params = {'max_tokens': 256, 'n_particles': 15, 'temperature': 1.0, 'grammar': json_grammar}\n",
    "    response = run_inference_server(prompt, **server_inference_params)\n",
    "    responses[sentence] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651cf6be-ddfc-4e91-9019-42ead761185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(responses.values())).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f00e1-7ea2-4883-8d48-ecf3fe93eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_html_output_text(text: str) -> str:\n",
    "    return text.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace(\"|\", \"\\|\").replace(\"\\n\", \"<br>\").replace(\"```\", \"\\\\```\")\n",
    "\n",
    "table_rows = ['| Sentence | Extracted Code | Raw Response | Aggregate Likelihood (Raw LH) |', '|:--- |:--- |:--- | ---:|']\n",
    "for sentence, response in responses.items():\n",
    "    try:\n",
    "        data = response.json()\n",
    "        posterior = data['posterior']\n",
    "        aggregate_likelihoods = get_aggregate_likelihoods(posterior)\n",
    "        # for i, (output_text, likelihood) in enumerate(posterior.items(), start=1):\n",
    "        #     # Assert the response parses properly\n",
    "        #     pclean_parser.parse(output_text)\n",
    "\n",
    "        #     code = extract_code_from_response(output_text)\n",
    "        #     aggregate_likelihood = aggregate_likelihoods[code]\n",
    "        #     code_with_br = code.replace(\"\\n\", \"<br>\")\n",
    "        #     output_with_br = md_html_output_text(output_text)\n",
    "        #     table_rows.append(\n",
    "        #         f'| {sentence if i == 1 else \"   \"} '\n",
    "        #         f'| <pre>{code_with_br}</pre> '\n",
    "        #         f'| <pre>{output_with_br}</pre> '\n",
    "        #         f'| {100 * aggregate_likelihood:.2f}% ({100 * likelihood:.2f}%) |'\n",
    "        #     )\n",
    "        for i, (code, likelihood) in enumerate(aggregate_likelihoods.items(), start=1):\n",
    "            aggregate_likelihood = aggregate_likelihoods[code]\n",
    "            code_with_br = code.replace(\"\\n\", \"<br>\")\n",
    "            table_rows.append(\n",
    "                f'| {sentence if i == 1 else \"   \"} '\n",
    "                f'| <pre>{code_with_br}</pre> '\n",
    "                f'| (unk) '\n",
    "                f'| {100 * aggregate_likelihood:.2f}% ({100 * likelihood:.2f}%) |'\n",
    "            )\n",
    "    except json.JSONDecodeError:\n",
    "        table_rows.append(\n",
    "            f'| {sentence if i == 1 else \"   \"} | ERROR: {responses.status_code} - {response.text} | 100% |'\n",
    "        )\n",
    "display(Markdown('\\n'.join(table_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31bb02-1b9d-4d73-9b98-f48a21b51f31",
   "metadata": {},
   "source": [
    "### Local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e3ed3a9-51f0-4553-ae3d-756423ce96ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "llama3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inference_setup \u001b[38;5;241m=\u001b[39m \u001b[43mserver_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_grammar\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m, in \u001b[0;36mserver_model\u001b[0;34m(grammar, proposal)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mserver_model\u001b[39m(grammar, proposal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharacter\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInferenceSetupVLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSERVER_GENPARSE_MODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposal_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproposal\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chix/genfactdemo/repos/genparse/genparse/backends/vllm/__init__.py:53\u001b[0m, in \u001b[0;36mInferenceSetupVLLM.__init__\u001b[0;34m(self, model_name, grammar, proposal_name, seed, guide_opts, proposal_opts, batch_size, temperature)\u001b[0m\n\u001b[1;32m     45\u001b[0m     llm \u001b[38;5;241m=\u001b[39m TokenizedLLM(\n\u001b[1;32m     46\u001b[0m         model\u001b[38;5;241m=\u001b[39mvllmpplLLM(MODEL_ID, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, max_model_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m),\n\u001b[1;32m     47\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_ID),\n\u001b[1;32m     48\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     49\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(model_name)\n\u001b[1;32m     55\u001b[0m guide \u001b[38;5;241m=\u001b[39m lark_guide(grammar, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mguide_opts)\n\u001b[1;32m     56\u001b[0m sampler \u001b[38;5;241m=\u001b[39m VLLMSampler(llm\u001b[38;5;241m=\u001b[39mllm, guide\u001b[38;5;241m=\u001b[39mguide)\n",
      "\u001b[0;31mValueError\u001b[0m: llama3"
     ]
    }
   ],
   "source": [
    "inference_setup = server_model(json_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c681a-4bbb-4895-81f6-23f9e3c23b5c",
   "metadata": {},
   "source": [
    "### Running the model on an arbitrary sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495e27c3-7021-4101-b51b-b006597a2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = {}\n",
    "for sentence_datum in tqdm([#{\"sentence\": \"John Smith the podiatrist is terrible, so much worse than John Smith the cardiologist.\"}, {\"sentence\": \"John Smith works with Jack Smith at the neurology office. He’s terrible at neurology.\"},\n",
    "                            {\"sentence\": \"John Smith's office (Happy Brain Services LLC) at 512 Example Street Suite 3600 (CA-170) is terrible!\"}]):\n",
    "    sentence = sentence_datum[\"sentence\"]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'user', 'content': format_json_prompt(sentence=sentence)}\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    logger.debug('Prompt (%d tokens): ```%s```', len(tokenizer(prompt)[\"input_ids\"]), prompt)\n",
    "\n",
    "    # ignored top_p=0.95\n",
    "    server_inference_params = {'max_tokens': 256, 'n_particles': 15, 'temperature': 1.0, 'grammar': json_grammar}\n",
    "    response = run_inference_server(prompt, **server_inference_params)\n",
    "    responses[sentence] = response\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54661c9b-43d5-4566-809b-e5952f9705d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_html_output_text_for_json(text: str) -> str:\n",
    "    return text.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace(\"|\", \"\\|\").replace(\"\\n\", \"<br>\").replace(\"```\", \"\\\\```\")\n",
    "\n",
    "table_rows = ['| Sentence | Extracted JSON | Raw Response | Aggregate Likelihood (Raw LH) |', '|:--- |:--- |:--- | ---:|']\n",
    "for sentence, response in responses.items():\n",
    "    try:\n",
    "        data = response.json()\n",
    "        posterior = data['posterior']\n",
    "        for i, (output_text, likelihood) in enumerate(posterior.items(), start=1):\n",
    "            # Assert the response parses properly\n",
    "            json_parser.parse(output_text)\n",
    "\n",
    "            #code = extract_code_from_response(output_text)\n",
    "            code = output_text\n",
    "            code_with_br = code.replace(\"\\n\", \"<br>\")\n",
    "            output_with_br = md_html_output_text_for_json(output_text)\n",
    "            table_rows.append(\n",
    "                f'| {sentence if i == 1 else \"   \"} '\n",
    "                f'| <pre></pre> '\n",
    "                f'| <pre>{output_with_br}</pre> '\n",
    "                f'| {100 * likelihood:.2f}% ({100 * likelihood:.2f}%) |'\n",
    "            )\n",
    "    except json.JSONDecodeError:\n",
    "        table_rows.append(\n",
    "            f'| {sentence if i == 1 else \"   \"} | ERROR: {responses.status_code} - {response.text} | 100% |'\n",
    "        )\n",
    "display(Markdown('\\n'.join(table_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e8bd2-8b01-445e-9872-d2cc081a8d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
